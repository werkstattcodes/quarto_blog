{"title":"Similarity of public submissions to Austria's amendment of the epidemic law","markdown":{"yaml":{"title":"Similarity of public submissions to Austria's amendment of the epidemic law","description":"An analysis of public submissions to bill seeking to amend Austria's epidemic law.","date":"12-22-2020","categories":["Austria","COVID","OCR","stringr","web scraping"],"image":"preview.png"},"headingText":"Setup","containsRefs":false,"markdown":"\n\n\n\n\n```{r echo=TRUE, message=FALSE, warning=FALSE}\n#| code-summary: \"Code: Load package\"\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(extrafont)\nloadfonts(device = \"win\", quiet = T)\nlibrary(hrbrthemes)\nhrbrthemes::update_geom_font_defaults(\n  family = \"Roboto Condensed\",\n  size = 3.5,\n  color = \"grey50\"\n)\nlibrary(scales)\nlibrary(knitr)\nlibrary(paletteer)\nlibrary(ggtext)\nlibrary(glue)\nlibrary(pdftools)\nlibrary(rvest)\nlibrary(janitor)\nlibrary(patchwork)\nlibrary(svglite)\nlibrary(countrycode)\nlibrary(quanteda.textstats)\nlibrary(tictoc)\nlibrary(furrr)\nlibrary(gt)\nlibrary(reactable)\nplan(multisession, workers = 3)\n\n```\n\n```{r setup, echo = T, include=F, eval=F}\n#| code-summary: \"Code: Define \"\n\n# knitr::opts_chunk$set(\n#  \tfig.align = \"left\",\n# \tmessage = FALSE,\n# \twarning = FALSE,\n#  \tdev = \"svglite\",\n# #\tdev.args = list(type = \"CairoPNG\"),\n# \tdpi = 300,\n#  \tout.width = \"100%\"\n# )\n# options(width = 180, dplyr.width = 150)\n```\n\n```{r echo=TRUE}\n#| code-summary: \"Code: Define plot theme, party colors, caption\"\n\nplot_bg_color <- \"white\"\n\ntheme_post <- function() {\n  hrbrthemes::theme_ipsum_rc() +\n    theme(\n      plot.background = element_rect(fill = plot_bg_color, color=NA),\n      panel.background = element_rect(fill = plot_bg_color, color=NA),\n      #panel.border = element_rect(colour = plot_bg_color, fill=NA),\n      #plot.border = element_rect(colour = plot_bg_color, fill=NA),\n      plot.margin = ggplot2::margin(l = 0, \n                           t = 0.25,\n                           unit = \"cm\"),\n      plot.title = element_markdown(\n        color = \"grey20\",\n        face = \"bold\",\n        margin = ggplot2::margin(l = 0, unit = \"cm\"),\n        size = 13\n      ),\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text(\n        color = \"grey50\",\n        margin = ggplot2::margin(t = 0.2, b = 0.3, unit = \"cm\"),\n        size = 11\n      ),\n      plot.caption = element_text(\n        color = \"grey50\",\n        size = 8,\n        hjust = c(0)\n      ),\n      plot.caption.position = \"panel\",\n      axis.title.x = element_text(\n        angle = 0,\n        color = \"grey50\",\n        hjust = 1\n      ),\n      axis.text.x = element_text(\n        size = 9,\n        color = \"grey50\"\n      ),\n      axis.title.y = element_blank(),\n      axis.text.y = element_text(\n        size = 9,\n        color = \"grey50\"\n      ),\n      panel.grid.minor.x = element_blank(),\n      panel.grid.major.x = element_blank(),\n      panel.grid.minor.y = element_blank(),\n      panel.spacing = unit(0.25, \"cm\"),\n      panel.spacing.y = unit(0.25, \"cm\"),\n      strip.text = element_text(\n        angle = 0,\n        size = 9,\n        vjust = 1,\n        face = \"bold\"\n      ),\n      legend.title = element_text(\n        color = \"grey30\",\n        face = \"bold\",\n        vjust = 1,\n        size = 7\n      ),\n      legend.text = element_text(\n        size = 7,\n        color = \"grey30\"\n      ),\n      legend.justification = \"left\",\n      legend.box = \"horizontal\", # arrangement of multiple legends\n      legend.direction = \"vertical\",\n      legend.margin = ggplot2::margin(l = 0, t = 0, unit = \"cm\"),\n      legend.spacing.y = unit(0.07, units = \"cm\"),\n      legend.text.align = 0,\n      legend.box.just = \"top\",\n      legend.key.height = unit(0.2, \"line\"),\n      legend.key.width = unit(0.5, \"line\"),\n      text = element_text(size = 5)\n    )\n}\n\ndata_date <- format(Sys.Date(), \"%d %b %Y\")\n\n```\n\n# Context\n\nAustria's government tabled about three months ago an amendment to the country's epidemic law. The amendment was largely a reaction to the shortcomings of the law dating from 1950 when confronted with the current Covid crisis as well as a recent ruling of Austria's Constitutional Court which declared several measures passed by the government as violating the constitution. Hence, the government proposed the amendment.\n\nWhile there would be a lot to say about the latent tension between civic rights and a state's obligation to curtail an epidemic or the thin line between legitimate restrictions and excessive infringements, this post will deal with the public submissions to the government's amendment bill. In short and crude terms, the legislative process in Austria provides the opportunity for citizens, NGOs, expert bodies etc to file submissions in which they can raise their concerns as to the (draft) version of the bill before it will be debated in parliament. So, at least in theory, it's an avenue for the government to solicit input from the public at large.\n\nWhat caught my eye, or better my ears, was a related radio news report which mentioned something along the lines that the submissions to the amendment reached a) a record number and b) that a considerable number of submissions were similar in their wording.\n\nI guess the former is indicative for the fundamental issues which the amendment touches and the overall somewhat 'edgy', not to say polarized atmosphere when it comes to Corona. The latter point, though, the similarity of submissions' wording, puzzled me a bit. What the similarity of the wording effectively means is the use of some kind of template by those filing a submission. In this regard, the news report mentioned that there have been some pertaining calls on social media channels to oppose the bill, possibly also including the provision of a template text.\n\nIt was this context which made me curious about the extent of the matter. At least curious enough to have an 'empirical' look at it with R.\n\nSo I went to the a) parliament's website which provides a record of received submissions, b) extracted the weblinks to all those submissions of which the text is public, c) download the pertaining pdfs, d) extracted their content, and e) finally checked for their similarity. As for the last point, I can gladly report that I learned something here and my approach changed a bit as I moved along. Hence, the blog post somewhat mirrors this process.\n\n```{r include=FALSE}\nsubmission_pattern <- \"Tür und Tor\"\n```\n\nWhen it comes to similarity, my first attempt was to simply look at the the recurrence of a distinct phrase which I repeatedly noticed while randomly glancing through some of the submissions. The formulation is *`r submission_pattern`* which means something like '(to open) the (flood)gates'. I guess it's safe to assume that those submissions articulated severe misgivings about the amendment.\n\nAs I'll show below, while this approach is already somewhat informative since it reveals a pervasive use of a rather distinct phrase and hence points towards the use of text template, it is rather inductive and likely to suffer from omissions of other, possibly even more re-current phrases. Furthermore, simply looking at one distinct phrase is likely to be a rather 'fragile' indicator. Only a minor, effectively non-substantive modification of the wording would result in missing out on similar submissions.\n\nTo make a long story short, this issue introduced me to the world of quantitative text analysis as provided by the powerful `quanteda` package. Admittedly, I am only starting to scratch the surface here, but hopefully deep enough to legitimately include it in this post. So, enough of context and waffling. In media R(es).\n\n# Get data\n\n## Links to submission subpages\n\nThe list of submissions filed for the amendment is provided [here](https://www.parlament.gv.at/PAKT/VHG/XXVII/ME/ME_00055/index.shtml#tab-Stellungnahmen){target=\"_blank\"} at the Parliament's website. To extract the links to the sub-pages which include the links to individual texts, I make us of the provided RSS feed. From there I extract the relevant elements and combine them to one dataframe containing the name of the person/institution filing a submission, the submission's date, and the link to the sub-page.\n\n```{r eval=FALSE, message=FALSE, warning=FALSE, include=TRUE, cache=T}\n#| code-summary: \"Code: get links to submission sub-pages\"\n\nlink_rss_all_submissions <- \"https://www.parlament.gv.at/PAKT/VHG/XXVII/ME/ME_00055/filter.psp?view=RSS&jsMode=&xdocumentUri=&filterJq=&view=&GP=XXVII&ITYP=ME&INR=55&SUCH=&listeId=142&FBEZ=FP_142\"\n\ndata <- xml2::read_xml(link_rss_all_submissions)\n\n#get link to subpages with link to submissions\ndf_submission_pages_link <- data %>% \n  xml2::xml_find_all(\"//link\") %>% \n  html_text() %>% \n  enframe(., \n          name=\"id\",\n          value=\"link_single_submission_page\") %>% \n  mutate(link_single_submission_page=str_squish(link_single_submission_page)) %>% \n  filter(id>2) #removes first two rows which don't include data on submissions\n\n#get title\ndf_submission_pages_name <- data %>% \n  xml2::xml_find_all(\"//title\") %>% \n  html_text() %>% \n  enframe(., \n          name=\"id\",\n          value=\"title\") %>% \n  mutate(name=str_extract(title, regex(\"(?<=\\\\>).*(?=\\\\<)\"))) %>% \n  filter(id>2) %>% \n  select(-title)\n\n#get publication date\ndf_submission_pages_pub_date <- data %>% \n  xml2::xml_find_all(\"//pubDate\") %>% \n  html_text() %>% \n  enframe(., \n          name=\"id\",\n          value=\"date\") %>% \n  mutate(date=date %>% str_squish() %>% lubridate::dmy_hms(., tz=\"Europe/Vienna\"))\n\n#combine to one dataframe\ndf_submission <- bind_cols(\n  df_submission_pages_name,\n  df_submission_pages_pub_date,\n  df_submission_pages_link,\n  ) %>% \n  select(-contains(\"id\"))\n```\n\n\n```{r eval=FALSE, include=FALSE}\nreadr::write_excel_csv2(df_submission, file=here::here(\"_blog_data\", \"2020-12-22-submissionsepidemiclaw\", \"df_submission_rss.csv\"))\n```\n\n```{r include=FALSE}\n# df_submission <- readr::read_csv2(file=\"df_submission_rss.csv\")\ndf_submission <- readr::read_csv2(file=here::here(\"posts\",\"2020-12-22-submissionsepidemiclaw\", \"df_submission_rss.csv\"))\n\n```\n\nAs it turns out, there were `r nrow(df_submission) %>% scales::comma()` submissions to the amendment in total.\n\n```{r echo=FALSE, paged.print=TRUE}\n#table.start\nreactable(df_submission, \n          defaultPageSize = 4, \n          columns = list(\n            link_single_submission_page = colDef(\n              cell = function(value) {\n                url <- value\n                htmltools::tags$a(href = url, \n                                  target = \"_blank\", \n                                  value)\n                },\n              width = 150,\n              html=TRUE\n              )),\n          filterable = TRUE,\n          bordered=TRUE,\n          compact = TRUE,\n          style = list(fontSize = \"10px\"),\n          theme = reactableTheme(\n            backgroundColor = plot_bg_color,\n                filterInputStyle = list(\n                  color=\"green\",\n                  backgroundColor = plot_bg_color)))\n#table.end\n```\n\nNotice that there are several submissions which state 'Keine öffentliche Stellungnahme' (no public statement) as a name. Hence, we will not be able to analyse the text of all submissions, but only of those which are made public. How many are there? Let's see:\n\n\n```{r message=FALSE, warning=FALSE}\n#| code-summary: \"Code: identify public and non-public submissions\"\n\ndf_submission <- df_submission %>% \n  mutate(public=case_when(str_detect(name, regex(\"Keine öffentliche Stellungnahme\",\n                                                 ignore_case = T)) ~ \"not public\",\n                          TRUE ~ as.character(\"public\")))\n\ntable(df_submission$public)\n```\n\n```{r echo=FALSE}\ntable(df_submission$public)\n```\n\nOut of `r nrow(df_submission) %>% scales::comma()` submissions, the text of `r df_submission %>% filter(public==\"public\") %>% nrow() %>% scales::comma()` is publicly available (no idea why some are published and others not, probably it's up to the submitter to decide). While the high number of non-public submissions is obviously quite a caveat for the further analysis, we still have the text of more than 50 % of all submissions to look into.\n\n## Links to pdfs\n\nNow let's use the links to the submissions' sub-pages and search in each sub-page for a link which leads to the actual pdf document of the submission. By using the browser's developer tools (F12 key), we can see the html code behind the page. Clicking on the link of one submission reveals that the link contains the word 'imfname'. I use this string to identify the links to the submissions' pdfs and neglect all other links and pdfs which are also on the site.\n\n![](identify_link.gif)\n\nTo do so, below the function which is applied to all (!) links leading to the sub-pages with the individual submissions. It reads the html code, extracts the links (href), converts the results into a dataframe, and subsequently filters out the links which include 'imfname'. I then complement the extracted link with the missing part of the web address (www.parlament....). Since there are several submission (sub-pages) which state that the text is not public, and don't include a link to a pdf, I insert in these cases a 'missing' instead of the link. To accelerate the process I apply the function in parallel by using \\`furrr::future\\_map\\`.\n\n```{r eval=FALSE, include=TRUE}\n#| code-summary: \"Code: read htmlof sub-pages, extract pdf-link\"\n\n# function to identify pdf link\nfn_get_doc_links <- function(x){ \n  \n  link_doc <- x %>% \n  read_html() %>% \n  html_nodes(\"a\") %>% \n  html_attr(\"href\") %>% \n  enframe(name=NULL,\n          value=\"link_part\") %>% \n  filter(str_detect(link_part, \"imfname\")) %>% \n  mutate(link_doc=paste0(\"https://www.parlament.gv.at\", link_part)) %>% \n        pull(link_doc)\n  ifelse(length(link_doc)>0, link_doc, \"missing\")\n  \n} \n\n#apply function to all links\ndf_submission <- df_submission %>% \n  mutate(link_doc=future_map_chr(link_single_submission_page, \n                          possibly(fn_get_doc_links,\n                                   otherwise=\"missing\"),\n                            .progress = T))\n```\n\n\n```{r eval=FALSE, include=FALSE}\n#check\ndf_submission %>% \n  filter(link_doc==\"missing\" & public==\"public\") %>% \n  nrow()\n#0 = ok; only links to non-public submissions are missing;\n\n#readr::write_excel_csv2(df_submission, path=here::here(\"data\", \"df_submission.csv\"))\n```\n\n## Download submissions\n\nNow with all pdf links available we can start to download them. To do so, I use the `walk2` function of the `purrr` package. Note that I also use `magrittr`'s `%$%`operator to expose/pipe into the the `walk2` function.\n\n```{r include=FALSE}\n# load saved links to pdf_files\n# df_submission <- readr::read_csv2(file=\"df_submission.csv\")\ndf_submission <- readr::read_csv2(file=here::here(\"posts\", \"2020-12-22-submissionsepidemiclaw\",\"df_submission.csv\"))\n\n```\n\n\n```{r eval=FALSE, include=TRUE}\n#| code-summary: \"Code: download files\"\n\n#define pdf's file name and download destination\ndf_submission<- df_submission %>% \n  mutate(file_name=str_extract(link_doc, regex(\"\\\\d+.pdf\"))) %>% \n  mutate(pdf_destination=glue::glue(\"{here::here('blog_data','2020-12-22-submissionsepidemiclaw','pdf_files')}/{file_name}\") %>% \n           as.character() %>% \n           str_trim(., side=c(\"both\")))\n\n#download\ndf_submission %>% \n  filter(link_doc!=\"missing\") %$% #exposes names\n  walk2(\n    link_doc,\n    pdf_destination,\n    download.file, \n    mode = \"wb\")\n```\n\n\nAfter the steps above we now have the all the available submission files at our disposal. Finally time to start working on the actual texts.\n\n## Extract text from pdfs\n\nWhen it comes to extracting text by means of OCR, `pdftools` by Jeroen Ooms is the package of choice. In preparation, we need to get the German language data. Extracting text from pdfs can be quite a time consuming matter. Going through the more than 3700 submissions initially took me more than 7 hours. Reducing the dpi value from 300 to 150 accelerated the procedure quite considerably, down to approx. 60 minutes, and I didn't notice any relevant decrease of the output's quality.\n\n\n```{r eval=FALSE, include=TRUE}\n#| code-summary: \"Code: extract text from pdfs\"\n\ntesseract::tesseract_download(lang = \"deu\")\ntesseract_info()\n\ndf_submission <- df_submission %>% \n   mutate(doc_text=map(pdf_destination, possibly(~pdftools::pdf_ocr_text(.,\n                                                                      language = \"deu\",\n                                                                      dpi=150),\n                                              otherwise=\"missing\")))\n```\n\n\n```{r include=FALSE}\ndf_submission <- readr::read_rds(file=here::here(\"posts\", \"2020-12-22-submissionsepidemiclaw\",\n                                                 \"df_submission_150.rds\"))\n\n```\n\nLet's clean up the imported text, i.e. collapse multi-page submissions into one single string text, remove leading and trailing white space etc.\n\n```{r}\n#| code-summary: \"Code: clean text\"\n\ndf_submission <- df_submission %>% \n  mutate(date=date %>% \n           str_squish() %>% \n           str_trim() %>% \n           lubridate::dmy_hms()) %>% \n  mutate(doc_text=map_chr(doc_text, paste, collapse=\" \")) %>% #collapse multipage docs\n  mutate(doc_text=str_squish(doc_text)) %>% \n  mutate(doc_text=stringr::str_trim(doc_text, side=c(\"both\"))) %>% \n  mutate(name_first=str_extract(name, regex(\"(?<=,\\\\s)[:alpha:]*(?=($|[^:alpha:]|\\\\s))\")), \n         .after=\"name\") \n```\n\n\nLet's pause for a moment and take stock: There are `r nrow(df_submission) %>% scales::comma()` submissions, out of which `r df_submission %>% filter(public==\"public\") %>% nrow() %>% scales::comma()` are public. From the latter we were able to extract `r df_submission %>% filter(doc_text!=\"missing\") %>% nrow() %>% scales::comma()`. Hence, `r df_submission %>% filter(public==\"public\") %>% nrow() - df_submission %>% filter(doc_text!=\"missing\") %>% nrow()` texts could not be retrieved.\n\n# Analysis\n\n## Identify submissions with key formulation\n\nNow with submissions' texts cleaned etc, let's identify those which contain the formulation which appeared quite frequently when randomly browsing through some of the texts: *`r submission_pattern`*.\n\n\n```{r include=T, eval=F}\n#| code-summary: \"Code: identify submission containing key phrase\"\nsubmission_pattern <- \"Tür und Tor\"\n```\n\n```{r}\ndf_submission <- df_submission %>% \n  mutate(pattern_indicator=stringr::str_detect(doc_text,\n                                                regex(pattern=submission_pattern,\n                                                      dotall=T,\n                                                      ignore_case=T,\n                                                      multiline=T))) %>%\n  mutate(overall_indicator=case_when(\n      public==\"not public\" ~ \"not public\",\n      pattern_indicator==TRUE ~ str_trunc(submission_pattern, 15),\n      TRUE ~ as.character(\"other\"))) %>% \n  mutate(doc_length=nchar(doc_text))\n```\n\n\n```{r}\n#| code-summary: \"Code: table with identified texts\"\n\ntb_text <- reactable(df_submission %>%\n            filter(pattern_indicator==TRUE) %>% \n            select(name, date, doc_text),\n          defaultPageSize = 4, \n          filterable = TRUE,\n          style = list(fontSize = \"10px\"),\n          columns=list(\n            name=colDef(width=100),\n            date=colDef(width=100),\n            doc_text=colDef(minWidth=200)),\n          bordered=TRUE,\n          compact = TRUE,\n          theme = reactableTheme(\n            backgroundColor = plot_bg_color,\n                filterInputStyle = list(\n                  color=\"green\",backgroundColor = plot_bg_color)\n          )\n          )\n\n```\n\n\n```{r echo=FALSE}\n#table.start\ntb_text\n#table.end\n```\n\n## Frequency of key formulation\n\n\n```{r message=FALSE, warning=FALSE}\n#| code-summary: \"Code: table\"\n# table -------------------------------------------------------------------\ndf_tb_submission_1 <- df_submission %>% \n  group_by(overall_indicator) %>% \n  summarise(n_abs=n()) %>% \n  mutate(n_rel=n_abs/sum(n_abs)) %>% \n  arrange(desc(n_abs)) %>% \n  mutate(indicator_tbl=case_when(overall_indicator==\"not public\" ~ \"Keine öffentliche Stellungnahmen\",\n                                 overall_indicator==str_trunc(submission_pattern, 15) ~ glue::glue(\"enthalten Formulierung '{submission_pattern}'\") %>% as.character(),\n                                 overall_indicator==\"other\" ~ \"andere öffentliche Stellungnahmen\"),\n         .before=1) %>% \n  select(-overall_indicator)\n```\n\n```{r message=FALSE, warning=FALSE}\ndf_tb_submission_2 <- df_submission %>% \n  group_by(overall_indicator) %>% \n  filter(overall_indicator!=\"not public\") %>% \n  summarise(n_abs=n()) %>% \n  mutate(n_rel=n_abs/sum(n_abs)) %>% \n  arrange(desc(n_abs)) %>% \n  mutate(indicator_tbl=case_when(overall_indicator==\"not public\" ~ \"Keine öffentliche Stellungnahmen\",\n                                 overall_indicator==str_trunc(submission_pattern, 15) ~ glue::glue(\"enthalten Formulierung '{submission_pattern}'\") %>% as.character(),\n                                 overall_indicator==\"other\" ~ \"andere öffentliche Stellungnahmen\"),\n         .before=1) %>% \n  select(-overall_indicator, -n_abs, n_rel_public=n_rel)\n```\n\n```{r message=FALSE, warning=FALSE}\ndf_tb_submission <- df_tb_submission_1 %>% \n  left_join(., df_tb_submission_2)\n```\n\n```{r message=FALSE, warning=FALSE}\ntb_submission <- df_tb_submission %>%   \n  gt() %>% \n  tab_header(\n    title = md(\"**Stellungnahmen zur Novelle des Epidemiegesetzes**\"),\n    subtitle = md(glue::glue(md(\" Von den {df_tb_submission %>% filter(!str_detect(indicator_tbl, 'Keine')) %>% summarise(n_abs=sum(n_abs)) %>% pull(n_abs) %>% scales::comma()} Stellungnahmen von denen der Text öffentlich ist enthalten rund {df_tb_submission %>% filter(str_detect(indicator_tbl, 'Formulierung')) %>% pull(n_rel_public) %>% scales::percent()} die Formulierung *'{submission_pattern}'*.\")))) %>% \n  tab_options(heading.align = \"left\",\n              row_group.font.weight = \"bold\",\n              table.background.color=plot_bg_color) %>% \n   tab_style(\n    style = list(\n      cell_fill(color = \"orange\"),\n      cell_text(weight = \"bold\")\n      ),\n    locations = cells_body(\n      columns = c(n_abs),\n      rows = n_abs == 2204) \n    )%>% \n       tab_style(\n        style = list(\n          cell_fill(color = \"orange\"),\n          cell_text(weight = \"bold\")\n        ),\n        locations = cells_body(\n        columns = c(n_abs, n_rel, n_rel_public),\n        rows = str_detect(indicator_tbl, \"Formulierung\")) \n    ) %>% \n  cols_label(indicator_tbl=\"Stellungnahme\",\n             n_abs=\"Anzahl\",\n             n_rel=\"Anteil alle St.\",\n             n_rel_public=\"Anteil öff. St.\") %>% \n  sub_missing(columns=c(n_rel_public), missing_text = \"--\") %>% \n  fmt_number(columns=c(n_abs), decimals=0) %>% \n  fmt_percent(columns = c(n_rel, n_rel_public), decimals = 1) %>% \n  tab_source_note(source_note = md(\"data: www.parlament.gv.at, 'Ministerialentwurf betreffend Bundesgesetz, mit dem das Epidemiegesetz 1950, das Tuberkulosegesetz und das COVID-19-Maßnahmengesetz geändert werden.'<br>analysis: Roland Schmidt | @zoowalk | https://werk.statt.codes\"))\n\n```\n\nAs the table below shows, in `r df_tb_submission %>% filter(str_detect(indicator_tbl, \"Formulierung\")) %>% pull(n_abs) %>% scales::comma()` submissions the formulation '*`r submission_pattern`*' appears. *This number amounts to `r df_tb_submission %>% filter(str_detect(indicator_tbl, \"Formulierung\")) %>% pull(n_rel) %>% scales::percent()` of all submissions, and to `r (df_tb_submission %>% filter(str_detect(indicator_tbl, \"Formulierung\")) %>% pull(n_abs)/(df_tb_submission %>% filter(str_detect(indicator_tbl, \"Formulierung\")) %>% pull(n_abs)+df_tb_submission %>% filter(str_detect(indicator_tbl, \"andere\")) %>% pull(n_abs))) %>% scales::percent()` of all submissions for which we can access the text.* I think that's quite remarkable in the sense that it is suggest for quite pervasive use of a submission template.\n\n```{r echo=FALSE, message=FALSE, warning=FALSE}\ntb_submission\n\n```\n\n## Distribution of length\n\nLet's approach the similarity of submissions from another angle now: The length of the text. The graph below shows the distribution of submissions' length in terms of number of characters. One density curve shows the distribution for submissions which include the key phrase '`r str_trunc(submission_pattern, 20)`', the other one shows the distribution of those which don't include it (and for which the text is available).\n\n```{r message=FALSE, warning=FALSE}\n#| code-summary: \"Code: distribution of length\"\n\n# distribution of length --------------------------------------------------\n\nmy_caption <- glue::glue(\"data: www.parlament.gv.at; Stellungnahmen zum Ministerialentwurf betreffend Bundesgesetz, mit dem das Epidemiegesetz 1950, das Tuberkulosegesetz \\nund das COVID-19-Maßnahmengesetz geändert werden.\\nanalysis: Roland Schmidt | @zoowalk | https://werk.statt.codes\")\n\npl_length <- df_submission %>% \n  filter(public!=\"not public\") %>%\n  filter(doc_length<1000) %>% #remove outliers\n  ggplot(., aes(y=pattern_indicator,\n             x=doc_length,\n             fill=pattern_indicator,\n             color=pattern_indicator)\n         )+\n  labs(title=paste(\"BEGUTACHTUNGSVERFAHREN EPIDEMIEGESETZ-NOVELLE:<br>\",\"Verteilung der Länge von Stellungnahmen mit u. ohne Formulierung \",\"'\",str_trunc(submission_pattern, 20),\"'\"),\n       subtitle=\"Zur besseren Sichtbarkeit wurden Stellungnahmen mit mehr als 1000 Zeichen ausgeschlossen.\",\n       x=\"Länge Stellungnahme in Zeichen\",\n       caption=my_caption)+\n  ggridges::geom_density_ridges(scale=5)+\n  scale_fill_manual(values=c(\"FALSE\"=\"darkolivegreen4\",\n                                   \"TRUE\"=\"orange\"),\n                    labels=c(\"FALSE\"=glue::glue(\"ohne '{str_trunc(submission_pattern, 20)}'\"),\n                             \"TRUE\"=glue::glue(\"mit '{str_trunc(submission_pattern, 20)}'\")))+\n  scale_color_manual(values=c(\"FALSE\"=\"darkolivegreen4\",\n                             \"TRUE\"=\"orange\"),\n                     labels=c(\"FALSE\"=glue::glue(\"ohne '{str_trunc(submission_pattern, 20)}'\"),\n                             \"TRUE\"=glue::glue(\"mit '{str_trunc(submission_pattern, 20)}'\")))+\n  scale_y_discrete(expand=expansion(mult=c(0, 0.1)))+\n  theme_post()+\n  theme(\n    plot.title.position = \"plot\",\n    panel.grid.minor.y = element_blank(),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    plot.caption = element_text(hjust=0),\n    legend.position = \"top\",\n    legend.justification = \"left\",\n   legend.direction = \"horizontal\",\n    legend.title=element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y=element_blank())+\n  guides(fill=guide_legend(reverse=T),\n         color=guide_legend(reverse=T))\n  \n```\n\n\n```{r echo=FALSE, dev='svg'}\npl_length\n```\n\nWhat the graph shows is that the length of submissions which contain the formulation '`r str_trunc(submission_pattern, 25)`' is concentrated at around 350 characters, while the length of other submissions is more dispersed. Hence, when it comes to the length of documents, those containing the distinct formulation are also pretty similar in term of their length. I take this as another indicator for the similarity of the submissions with our search phrase. As for the density curve of those not containing the search phrase, note that there is still a little 'bump' around 350. This could potentially reflect that we are missing out on some submissions which may originate from the same submission template, but do not include the exact wording of our search phrase.\n\n## Submissions over time\n\nI was also wondering whether there is any 'temporal' pattern as to the date of the submission. If the submissions were the result of a social media campaign, are the timings of their submissions clustered? This could be read as a reaction to e.g. a social media post providing a template and asking to submit it.\n\n```{r message=FALSE, warning=FALSE}\n#| code-summary: \"Code: submissions per day\"\n# bar graph ---------------------------------------------------------------\n\ndf_submission_pattern <- df_submission %>% \n  mutate(overall_indicator=fct_infreq(overall_indicator)) %>% \n  group_by(date, overall_indicator, .drop=F) %>% \n  summarise(n=n()) %>% \n  ungroup() %>% \n  mutate(date=as.Date(date))\n\nlevels(df_submission_pattern$overall_indicator)\n\nlabel_submission_pattern <- as.character(glue::glue(\"Enthält '{str_trunc(submission_pattern,15)}'\"))\n\ncolors_indicators=c(\"grey70\", \"orange\", \"darkolivegreen4\")\nnames(colors_indicators) <- c(\"not public\", str_trunc(submission_pattern, 15), \"other\")\n\npl_submission_pattern <- df_submission_pattern %>% \n  filter(date<=as.Date(\"2020-09-19\")) %>% \n  ggplot()+\n  labs(title=paste(\"BEGUTACHTUNGSVERFAHREN EPIDEMIEGESETZ-NOVELLE:<br>\",\"Zeitpunkt der Einrechung der Stellungnahmen, mit u. ohne Formulierung\"),\n       subtitle=\"\",\n       caption=my_caption,\n       x=\"Datum\",\n       y=\"Anzahl Einreichungen\")+\n  geom_bar(aes(x=date,\n               y=n,\n               fill=overall_indicator),\n           position=position_dodge2(),\n           color=NA,\n           stat=\"identity\",\n           key_glyph = \"dotplot\")+\n  scale_x_date(labels=scales::label_date_short(),\n               date_breaks=\"1 days\")+\n  scale_y_continuous(expand=expansion(mult=c(0,0.1)),\n                     labels = scales::label_comma())+\n  scale_fill_manual(values=colors_indicators,\n                    labels=c(\"not public\"=\"keine öffentliche Stellungnahme\",\n                             submission_pattern=label_submission_pattern,\n                                  \"other\"=\"andere öffentliche Stellungnahmen\"))+\n  theme_post()+\n  theme(\n    plot.title.position = \"panel\",\n    plot.subtitle = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    plot.caption = element_text(hjust=0),\n    legend.position = \"top\",\n    legend.box.margin = ggplot2::margin(l=0, unit=\"cm\"),\n    legend.margin = ggplot2::margin(l=0, unit=\"cm\"),\n    legend.key.size = unit(1, units=\"cm\"),\n    legend.justification = \"left\",\n    legend.title=element_blank(),\n    axis.title.x = element_blank()\n  )+\n  guides(fill=guide_legend(size=15))\n```\n\n\n```{r echo=FALSE, dev='svg'}\npl_submission_pattern\n```\n\nI think the bar chart above doesn't suggest any temporal clustering of submissions including '`r submission_pattern`'. But considering that the speed with which the amendment was introduced and the very short period to file submissions doesn't really allow for much clustering.\n\n# Analysis with 'quanteda'\n\nAs already indicated in the introduction, the above analysis - while informative to some extent - has its limitations. Limiting the search to the presence/absence of the key phrase '`r submission_pattern`' is prone to miss out on other wordings which might be even more prevalent in some of the submissions. Furthermore, only marginal changes to the wording could result in the omission of submissions which are otherwise very similar.\n\nTo overcome these limitations and approach the issue of text similarity in a methodologically more rigors manner, let's now take up some of the features of the `quanteda` package. I never worked with the package before but from what I can tell it provides a very comprehensive and powerful set of tools for quantitative text analysis (for an overview see [here](https://quanteda.io/articles/pkgdown/comparison.html){target=\"_blank\"}). \n\nTo be able to use the offered tools, we have to convert our collection of submission texts into a 'corpus'. Before doing so, I further clean the extracted texts so that we only have the actual text of the submission and not any header, footer, or meta-data. Then let's create a `corpus`.\n\n\n```{r}\n#| code-summary: \"Code: Clean text, create corpus\"\nlibrary(quanteda)\ndf_submission_quanteda <- df_submission %>% \n  filter(doc_text!=\"missing\") %>% \n  select(file_name, doc_text) %>% \n  mutate(doc_text=str_remove(doc_text,regex(\"^.*Eingebracht am: \\\\d{2}\\\\.\\\\d{2}\\\\.\\\\d{4}\\\\s*\"))) %>% \n  mutate(doc_text=str_remove(doc_text,regex(\"\\\\s?www.parlament.gv.at\\\\s*$\"))) %>% \n  mutate(doc_text=str_remove(doc_text,regex(\"^.?Betr\\\\.?\\\\:?\\\\s?Änderung(en)? des Ep(i|e)demie Gesetzes\\\\.?\"))) %>% \n  mutate(doc_text=doc_text %>% str_squish() %>% str_trim()) %>% \n  ungroup()\n\n#create vector\nvec_submission <- df_submission_quanteda %>% \n  deframe()\n\n#create corpus\ncorp_submission <- corpus(vec_submission, docvars=data.frame(submission=names(vec_submission)))\n```\n\n\nThe corpus is essentially a named vector with the names of the submission files the names of the vector's elements.\n\n```{r echo=FALSE}\nhead(corp_submission)\n```\n\n## Collocations\n\nWith this as the basis, we can now identify all collocations e.g. which comprise `r stringi::stri_count_words(submission_pattern)` words as e.g. in '`r submission_pattern`'. The `textstat_collocation` function provides us with the frequencies for all (!) collocations of the requested length. For performance reasons I limited the results to those collocations which result in more than 500 hits. For a detailed description of the function see [here](https://quanteda.io/reference/textstat_collocations.html){target=\"_blank\"}.\n\n\n```{r}\n#| code-summary: \"Code: Get collocations\"\n\n#collocations\n\nn_words <- str_count(submission_pattern, regex(\"\\\\S+\"))\nn_words\n\n\ncollocs <- textstat_collocations(\n  corp_submission,\n  method = \"lambda\",\n  size = n_words,\n  min_count = 500,\n  smoothing = 0.5,\n  tolower = T) %>% \n  arrange(desc(count))\n\ndf_collocs <- collocs %>% \n  as_tibble()\n\n```\n\n\nAnd as it turns out, our used inductively selected key phrase '`r submission_pattern`' is close to the most frequent one. The collocation '`r df_collocs %>% slice_head(n=1) %>% pull(collocation)`' appears `r df_collocs %>% slice_head(n=1) %>% pull(count) %>% scales::comma()`.[^1]\n\n[^1]: Note that the frequency of `r submission_pattern` quanteda result is different than the frequency I detected earlier. I am not entirely sure why this is, but I assume it's due to different approaches. While our result above is the actual\n\n```{r echo=FALSE}\n#table.start\nreactable(df_collocs,\n          bordered=TRUE,\n          compact = TRUE,\n          style = list(fontSize = \"10px\"),\n          filterable = TRUE,\n          theme = reactableTheme(\n            backgroundColor = plot_bg_color,\n                filterInputStyle = list(\n                  color=\"green\",\n                  backgroundColor = plot_bg_color)))\n#table.end\n```\n\n## Cosine similarity\n\nQuantitative text analysis offers also the concept of **cosine similarity** when it comes to evaluating the similarity of two or more documents. I won't dig into the details here, but want to refer you to the video below which provides in my view a clear and accessible overview.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/5lvS8078ykA\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n<br>\n\nOne of the convenient properties of the concept is that it allows us to compare documents of any lengths along a scale of 0 (not similar at all) to 1 (identical).\n\nIn order to control for different document length when it comes to check for similarity, we have to create a weighted document feature matrix which accounts for the relative frequency of words (see [here](https://quanteda.io/reference/dfm_weight.html?q=dfm%20_%20weight){target=\"_blank\"}).\n\n\n```{r}\n  #| code-summary: \"Code: create document feature matrix (dfm)\"\n#create document-feature matrix\n\nvec_submission_wo_pattern <- map_chr(vec_submission, ~str_remove_all(., submission_pattern))\n\ncorp_submission_wo_term <- corpus(vec_submission_wo_pattern, docvars=data.frame(submission=names(vec_submission_wo_pattern)))\n\n\ndfmat_submission <- dfm(corp_submission_wo_term, \n                        remove = stopwords(\"de\"),\n                        stem = TRUE, \n                        remove_punct = TRUE)\n\ndfmat_prop_submission <- dfm_weight(dfmat_submission,\n                                    scheme=\"prop\")\n```\n\nWith this we can calculate the cosine similarity of texts.\n```{r}\n#| code-summary: \"Code: create similarity matrix (dfm)\"\n#create similarity matrix\nmat_similarity <- textstat_simil(dfmat_prop_submission, dfmat_prop_submission, \n                            margin = \"documents\", \n                            method = \"cosine\")\n\n#convert to dataframe\ndf_similarity <- mat_similarity %>% \n  as.data.frame(., \n                upper=FALSE,\n                diag=FALSE,\n  ) %>% \n  mutate(sim_interval=cut(cosine, c(seq(0,1,.1)),\n                          right=F,\n                          include.lowest=T))\n```\n\n\n### Document pairwise comparison\n\nWhat we obtain now is a comparison of all submission pairs (!) as to their cosine similarity. Overall, these are `r df_similarity %>% nrow() %>% scales::comma()` pairs (duplicates removed, i.e. document 1 vs document 2 = document 2 vs document 1). Seems like text analysis can quickly get a bit large. To keep the table manageable I only show the first thousand.\n\n```{r echo=FALSE}\n#table.start\nreactable(head(df_similarity, n = 10^3),\n          columns=list(\n            cosine=colDef(format=colFormat(digits=4),\n                          align=\"left\")),\n          bordered=TRUE,\n          compact = TRUE,\n          style = list(fontSize = \"10px\"),\n          filterable = TRUE,\n          theme = reactableTheme(\n            backgroundColor = plot_bg_color,\n                filterInputStyle = list(\n                  color=\"green\",\n                  backgroundColor = plot_bg_color)))\n#table.end\n\n```\n\nTo be able to group these pairs according to their similarity, I cut their cosine similarity values into 10 intervals (`sim_interval`). We can now count the number of submission pairs per interval. The barplot below presents the relative result, i.e. the share of submission pairs  from the total number of pairs.\n\n\n```{r}\n#| code-summary: \"Code: Frequency of pairs per cosine interval \"\n\ndf_similarity <- df_similarity %>% \n  left_join(., df_submission %>% \n              select(file_name, pattern_indicator),\n            by=c(\"document1\"=\"file_name\")) %>% \n  rename(document1_indicator=pattern_indicator) %>% \n  left_join(., df_submission %>% \n              select(file_name, pattern_indicator),\n            by=c(\"document2\"=\"file_name\")) %>% \n  rename(document2_indicator=pattern_indicator) %>% \n  mutate(indicator=case_when(document1_indicator==T & document2_indicator==T ~ \"both submissions with key phrase\",\n                             document1_indicator==F & \n                               document2_indicator==F ~ \"both submissions without key phrase\",\n                             TRUE ~ as.character(\"one submission without key phrase\")))\n\nvec_color_submissions <- c(\"#FFC125\", \"#104E8B\", \"grey50\")\nnames(vec_color_submissions) <- c(\"both submissions with key phrase\",\n\"one submission without key phrase\",\n\"both submissions without key phrase\")\n\n#barplot\ndf_bar_sim <- df_similarity %>% \n  group_by(sim_interval, indicator) %>% \n  summarise(interval_n=n()) %>% \n  ungroup() %>% \n  mutate(interval_rel=interval_n/sum(interval_n)) %>% \n  ungroup() %>% \n  mutate(law=\"EpidemieG\") %>% \n  mutate(indicator=fct_relevel(indicator, names(vec_color_submissions)))\n\n\npl_bar_sim <- df_bar_sim %>% \n  ggplot() +\n  labs(\n    title=\"PUBLIC CONSULTATION PROCEDURE ON EPIDEMIC LAW:<br>\n    Distribution of all submission pairs' cosine similarity\",\n    subtitle=\"\",\n    y=\"% of all submission pairs\",\n    x=\"cosine similarity\",\n    caption=my_caption)+\n  geom_bar(aes(x=sim_interval,\n               y=interval_rel,\n               fill=indicator),\n           position=position_stack(),\n           stat=\"identity\")+\n  geom_text(\n    # data=. %>%\n    #           filter(sim_interval %in% c(\"[0.7,0.8)\", \"[0.8,0.9)\", \"[0.9,1]\")) %>%\n    #                    filter(str_detect(indicator, \"both submissions with key phrase\")),\ndata=. %>%  mutate(interval_rel_label=case_when(\n                                                indicator==\"both submissions without key phrase\" ~ NA_real_,\n  interval_rel>0.05 ~ interval_rel,\n                                                TRUE ~ NA_real_)),\n    aes(x=sim_interval,\n    y=interval_rel,\n    label=interval_rel_label %>% scales::percent(., accuracy = 0.01),\n    group=indicator),\nposition=position_stack(vjust=0.5, reverse = F),\ncolor=\"white\")+\n  # stat_summary(fun.y = sum, \n  #              aes(x=sim_interval,\n  #                  y=interval_rel+.1,\n  #                  label = ..y.. %>% scales::percent(., accuracy = 0.01), \n  #                  group = sim_interval), \n  #              # position=position_stack(),\n  #              # vjust=2,\n  #              geom = \"text\")+\n  geom_text(\n    aes(x=sim_interval,\n        y=interval_rel,\n        label = ..y.. %>% scales::percent(., accuracy = 0.01),\n        group = sim_interval), \n    stat = 'summary', \n    fun = sum, \n    vjust = -1\n  )+\n  scale_y_continuous(labels=scales::label_percent(accuracy = 1),\n                     expand=expansion(mult=c(0, 0.1)))+\n  scale_fill_manual(values=vec_color_submissions)+\n  theme_post()+\n  theme(axis.title.y = element_text(angle = 90,\n                                    size=9,\n                                    color=\"grey50\",\n                                    hjust=1),\n        legend.direction = \"horizontal\",\n        legend.position=\"top\",\n        plot.subtitle = element_blank(),\n        plot.title=element_markdown(margin=unit(0.5, \"cm\")),\n        legend.title=element_blank(),\n        legend.text = element_text(size=9))\n\n```\n\n```{r echo=FALSE, dev='svg'}\npl_bar_sim\n```\n<br>\nIf we take 0.7 as a cut-off point, we can conclude that `r scales::percent(df_similarity %>% filter(cosine>=.7) %>% nrow()/nrow(df_similarity), accuracy=0.01)` of all submission pairs (!) were very similar. Note that among these submission pairs, there is no pair in which not at least one document contains our key phrase *'`r submission_pattern`'*. On the other end of the scale, we see that the largest junk of non-similar pairs (`r df_bar_sim %>% filter(sim_interval==\"[0,0.1)\" & str_detect(indicator, \"one submission without\")) %>% pull(interval_rel) %>% scales::percent(accuracy=0.01)` in the interval [0, 0.1)) consists of pairs of one document with the key phrase and the other document without it. \n\nNote that I removed the key phrase before calculating the cosine similarity. Hence, the high degree of similarity of submission pairs containing the key phrase does not originate from the key phrase, but from the rest of the texts! \n\n```{r}\n#| code-summary: \"Code: Plot composition per cosine interval\"\n\ndf_bar_sim %>% \n  group_by(sim_interval, indicator) %>% \n              summarise(sum_interval=sum(interval_rel, na.rm=T)) %>% \n              mutate(sum_indidicator_rel=sum_interval/sum(sum_interval))\n```\n\nAnother way to approach the issue is to look into the composition of each interval. The plot below provides the pertaining overview. As it becomes quite clear, the the share of submission pairs with both documents containing our search phrase becomes higher the more similar the documents are. Again, let's note that the detected similarity does not originate from both documents containing '`r submission_pattern`' since I removed these words before calculating pairs' similarity. I take plot below as another indication of the use of a submission template. \n\n```{r message=FALSE, warning=FALSE}\npl_bar_sim_fill <- df_bar_sim %>% \n  ggplot() +\n  labs(\n    title=\"PUBLIC CONSULTATION PROCEDURE ON EPIDEMIC LAW:<br>\n    Composition of submission pairs' cosine similarity intervals\",\n    # subtitle=\"\",\n    y=\"% of all submission pairs\",\n    x=\"cosine similarity\",\n    caption=my_caption)+\n  geom_bar(aes(x=sim_interval,\n               y=interval_rel,\n               fill=indicator),\n           position=\"fill\",\n           stat=\"identity\")+\n  # geom_text(aes(x=sim_interval,\n  #               y=interval_rel,\n  #               group=indicator,\n  #               label=interval_rel %>% scales::percent(., accuracy = 0.01)),\n  #           position=position_fill(reverse=FALSE, vjust=0.5),\n  #           color=\"white\")+\n  geom_text(data=. %>% \n              group_by(sim_interval, indicator) %>% \n              summarise(sum_interval=sum(interval_rel, na.rm=T)) %>% \n              mutate(sum_indidicator_rel=sum_interval/sum(sum_interval)) %>% \n              mutate(sum_indidicator_rel=case_when(sum_indidicator_rel<0.05 ~ NA_real_,\n                                                   TRUE ~ as.numeric(sum_indidicator_rel))),\n            aes(x=sim_interval,\n                y=sum_indidicator_rel,\n                group=indicator,\n                label=sum_indidicator_rel %>% scales::percent(., accuracy=0.1)),\n            color=\"white\",\n            position=position_fill(vjust=0.5))+\n  geom_text(data=df_similarity %>% \n              group_by(sim_interval) %>% \n              summarise(n_obs=n()),\n            aes(x=sim_interval,\n                y=1.1,\n                label=n_obs %>% scales::comma()),\n            color=\"grey40\",\n            stat=\"identity\")+\n  geom_text(label=\"number of pairs:\",\n            x=.5,\n            y=1.17,\n            color=\"grey40\",\n            hjust=0,\n            check_overlap = T)+\n  scale_y_continuous(labels=scales::label_percent(accuracy=1),\n                     expand=expansion(mult=c(0, 0.1)),\n                     breaks=seq(0,1,.25))+\n  scale_fill_manual(values=vec_color_submissions)+\n  theme_post()+\n  theme(axis.title.y = element_text(angle = 90,\n                                    size=9,\n                                    color=\"grey50\",\n                                    hjust=1),\n        legend.position=\"top\",\n        legend.direction = \"horizontal\",\n        legend.text=element_text(size=9),\n        plot.subtitle = element_blank(),\n        plot.title=element_markdown(margin=unit(0.5, \"cm\")),\n        legend.title=element_blank())\n```\n\n\n```{r echo=FALSE, dev='svg'}\npl_bar_sim_fill\n```\n\n### Groupwise comparison\n\nFinally, to further highlight the similarity of documents with `r submission_pattern` in contrast to other submission pairs, let's contrast the distribution of of the cosine similarity values per group with a violin plot.\n\n\n```{r}\n#| code-summary: \"Code: groupwise comparison\"\n\n#create indicator for each document\ndf_submission <- df_submission %>%\n  mutate(search_term=str_detect(doc_text, regex(submission_pattern,\n                                                ignore_case = T,\n                                                multiline = T,\n                                                dotall = T))) \n\n#add indicator to pairwise similarity comparison for first document\ndf_similarity <- df_similarity %>% \n  left_join(.,\n            df_submission %>% \n              select(file_name, search_term),\n            by=c(\"document1\"=\"file_name\")) %>% \n  rename(document1_term=search_term)\n\n#add indicator to pairwise similarity comparison for first document\ndf_similarity <- df_similarity %>% \n  left_join(.,\n            df_submission %>% \n              select(file_name, search_term),\n            by=c(\"document2\"=\"file_name\")) %>% \n  rename(document2_term=search_term)\n\n#create one, both, none groups\ndf_similarity <- df_similarity %>% \n  mutate(group_id=case_when(\n    document1_term==TRUE & document2_term==TRUE ~ \"both\",\n    document1_term==FALSE & document2_term==FALSE ~ \"none\",\n    document1_term==TRUE & document2_term==FALSE ~ \"one\",\n    document1_term==FALSE & document2_term==TRUE ~ \"one\",\n    TRUE ~ as.character(\"missing\")))\n```\n\n```{r}\npl_sim_groups <- df_similarity %>% \n  mutate(indicator=fct_relevel(indicator, names(vec_color_submissions))) %>% \n  ggplot()+\n  labs(\n    title=\"PUBLIC CONSULTATION PROCEDURE ON EPIDEMIC LAW:<br>Cosine similarity of submission pairs per group\",\n    subtitle=str_wrap(glue::glue(\"Grouped by presence/absence of key phrase '{submission_pattern}'. Key phrase was removed before calculating cosine similarity.\"),110),\n    y=\"cosine similarity\",\n       caption=my_caption)+\n  geom_violin(aes(x=indicator,\n                   y=cosine,\n                  fill=indicator),\n              color=NA)+\n  theme_post()+\n  theme(legend.position = \"none\",\n        axis.title.x = element_blank())+\n  scale_fill_manual(values=vec_color_submissions)\n\n```\n\n\n\n```{r echo=FALSE, dev='svg'}\npl_sim_groups\n```\n\nI think the result shows quite clearly that those submissions which include the key phrase are much more similar to each other than all other groups of submission pairs even after having removed the key phrase. The graph also shows that those submission which do not contain the wording are hardly similar to each other.\n\n## Comparision with other bills\n\nBut how does this result contrast with e.g. submissions to other bills. To put the results for the epidemic bill into context, I repeated the analysis from above for another bill, the 2018/19 Fundamental Law on Social Welfare ('[Sozialhilfegrundgesetz](https://www.parlament.gv.at/PAKT/VHG/XXVI/ME/ME_00104/index.shtml#tab-Stellungnahmen){target=\"_blank\"}').[^2] The plot below contrasts the distribution of cosine similarities between submission pairs. \n\n[^2]: I don't reproduce the code for the analysis here, but it is available on my github.\n\n```{r echo=FALSE}\ndf_sozial_sim <- readr::read_csv2(file=here::here(\"posts\", \"2020-12-22-submissionsepidemiclaw\", \"df_bar_sim_sozial.csv\")) %>%  mutate(law=\"Sozialhilfegrundgesetz\")\n\ndf_sim_comb <- bind_rows(df_sozial_sim, df_bar_sim) %>% \n  group_by(law, sim_interval) %>% \n  summarise(interval_rel=sum(interval_rel, na.rm=T))\n\npl_sim <- df_sim_comb %>% \n  ungroup() %>% \n  ggplot() +\n  labs(title=\"PUBLIC CONSULTATION PROCEDURE:<br>Cosine similarity of submissions to \n        <span style=color:#FFC125>Epidemic Law</span> vs \n       <span style=color:firebrick '>Law on Social Welfare</span>\",\n       subtitle=\"Epidemic law = Epidemiegesetz; Law on Social Welfare = Sozialhilfegrundgesetz.\",\n    y=\"% of all submission pairs\",\n    x=\"cosine similarity\",\n    caption=glue::glue(str_wrap(\"data: www.parlament.gv.at; Stellungnahmen zum 'Ministerialentwurf betreffend Bundesgesetz, mit dem das Epidemiegesetz 1950, das Tuberkulosegesetz \nund das COVID-19-Maßnahmengesetz geändert werden' und 'Ministerialentwurf betreffend Bundesgesetze, mit dem ein Bundesgesetz betreffend Grundsätze für die Sozialhilfe (Sozialhilfe-Grundsatzgesetz) und ein Bundesgesetz über die bundesweite Gesamtstatistik über Leistungen der Sozialhilfe (Sozialhilfe-Statistikgesetz) erlassen werden.'\", 140), \"\\n\\nanalysis: Roland Schmidt | @zoowalk | https://werk.statt.codes\")) +\n  geom_bar(aes(x=sim_interval,\n               fill=law,\n               y=interval_rel),\n           position=position_dodge2(),\n           stat=\"identity\")+\n  scale_y_continuous(labels=scales::label_percent(),\n                     expand=expansion(mult=c(0, 0.1)))+\n  # scale_fill_paletteer_d(\"ggsci::default_jama\")[2:3]  +\n  scale_fill_manual(values=c(\"#FFC125\", \"firebrick\"))+\n  theme_post()+\n  theme(axis.title.y = element_text(angle = 90,\n                                    size=9,\n                                    color=\"grey50\",\n                                    hjust=1),\n        plot.title.position = \"panel\",\n        legend.position = \"none\",\n        legend.direction = \"horizontal\",\n        legend.title = element_blank())\n```\n\n```{r echo=FALSE, dev='svg'}\npl_sim\n```\n<br>\nI think the result is quite telling. In contrast to  epidemic law, the values pertaining to the Law on Social Welfare are almost normally distributed and centered on the [0.3, 0.4) interval. In other words, there almost no submissions which are particularly similar or dissimilar. I would think that this is a distribution typical for submissions without the (prevalent) presence of a text template. \n\n\n# Wrap up\n\nSo that's it (for now). Again, this got more lengthy than initially intended. A next step could be to see whether the similarity to submissions grew since the ability to file submissions via email. After all, copying a template, modifying it a bit before submission is not too difficult. Maybe as a final word, this is not to say that copy-and-paste submissions are of some sort of diminished democratic value. As always, if you have any comment or question regarding the post, feel free to contact me via DM on [twitter.](https://twitter.com/zoowalk){target=\"_blank\"}.\n\n\n","srcMarkdownNoYaml":"\n\n\n\n# Setup\n\n```{r echo=TRUE, message=FALSE, warning=FALSE}\n#| code-summary: \"Code: Load package\"\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(extrafont)\nloadfonts(device = \"win\", quiet = T)\nlibrary(hrbrthemes)\nhrbrthemes::update_geom_font_defaults(\n  family = \"Roboto Condensed\",\n  size = 3.5,\n  color = \"grey50\"\n)\nlibrary(scales)\nlibrary(knitr)\nlibrary(paletteer)\nlibrary(ggtext)\nlibrary(glue)\nlibrary(pdftools)\nlibrary(rvest)\nlibrary(janitor)\nlibrary(patchwork)\nlibrary(svglite)\nlibrary(countrycode)\nlibrary(quanteda.textstats)\nlibrary(tictoc)\nlibrary(furrr)\nlibrary(gt)\nlibrary(reactable)\nplan(multisession, workers = 3)\n\n```\n\n```{r setup, echo = T, include=F, eval=F}\n#| code-summary: \"Code: Define \"\n\n# knitr::opts_chunk$set(\n#  \tfig.align = \"left\",\n# \tmessage = FALSE,\n# \twarning = FALSE,\n#  \tdev = \"svglite\",\n# #\tdev.args = list(type = \"CairoPNG\"),\n# \tdpi = 300,\n#  \tout.width = \"100%\"\n# )\n# options(width = 180, dplyr.width = 150)\n```\n\n```{r echo=TRUE}\n#| code-summary: \"Code: Define plot theme, party colors, caption\"\n\nplot_bg_color <- \"white\"\n\ntheme_post <- function() {\n  hrbrthemes::theme_ipsum_rc() +\n    theme(\n      plot.background = element_rect(fill = plot_bg_color, color=NA),\n      panel.background = element_rect(fill = plot_bg_color, color=NA),\n      #panel.border = element_rect(colour = plot_bg_color, fill=NA),\n      #plot.border = element_rect(colour = plot_bg_color, fill=NA),\n      plot.margin = ggplot2::margin(l = 0, \n                           t = 0.25,\n                           unit = \"cm\"),\n      plot.title = element_markdown(\n        color = \"grey20\",\n        face = \"bold\",\n        margin = ggplot2::margin(l = 0, unit = \"cm\"),\n        size = 13\n      ),\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text(\n        color = \"grey50\",\n        margin = ggplot2::margin(t = 0.2, b = 0.3, unit = \"cm\"),\n        size = 11\n      ),\n      plot.caption = element_text(\n        color = \"grey50\",\n        size = 8,\n        hjust = c(0)\n      ),\n      plot.caption.position = \"panel\",\n      axis.title.x = element_text(\n        angle = 0,\n        color = \"grey50\",\n        hjust = 1\n      ),\n      axis.text.x = element_text(\n        size = 9,\n        color = \"grey50\"\n      ),\n      axis.title.y = element_blank(),\n      axis.text.y = element_text(\n        size = 9,\n        color = \"grey50\"\n      ),\n      panel.grid.minor.x = element_blank(),\n      panel.grid.major.x = element_blank(),\n      panel.grid.minor.y = element_blank(),\n      panel.spacing = unit(0.25, \"cm\"),\n      panel.spacing.y = unit(0.25, \"cm\"),\n      strip.text = element_text(\n        angle = 0,\n        size = 9,\n        vjust = 1,\n        face = \"bold\"\n      ),\n      legend.title = element_text(\n        color = \"grey30\",\n        face = \"bold\",\n        vjust = 1,\n        size = 7\n      ),\n      legend.text = element_text(\n        size = 7,\n        color = \"grey30\"\n      ),\n      legend.justification = \"left\",\n      legend.box = \"horizontal\", # arrangement of multiple legends\n      legend.direction = \"vertical\",\n      legend.margin = ggplot2::margin(l = 0, t = 0, unit = \"cm\"),\n      legend.spacing.y = unit(0.07, units = \"cm\"),\n      legend.text.align = 0,\n      legend.box.just = \"top\",\n      legend.key.height = unit(0.2, \"line\"),\n      legend.key.width = unit(0.5, \"line\"),\n      text = element_text(size = 5)\n    )\n}\n\ndata_date <- format(Sys.Date(), \"%d %b %Y\")\n\n```\n\n# Context\n\nAustria's government tabled about three months ago an amendment to the country's epidemic law. The amendment was largely a reaction to the shortcomings of the law dating from 1950 when confronted with the current Covid crisis as well as a recent ruling of Austria's Constitutional Court which declared several measures passed by the government as violating the constitution. Hence, the government proposed the amendment.\n\nWhile there would be a lot to say about the latent tension between civic rights and a state's obligation to curtail an epidemic or the thin line between legitimate restrictions and excessive infringements, this post will deal with the public submissions to the government's amendment bill. In short and crude terms, the legislative process in Austria provides the opportunity for citizens, NGOs, expert bodies etc to file submissions in which they can raise their concerns as to the (draft) version of the bill before it will be debated in parliament. So, at least in theory, it's an avenue for the government to solicit input from the public at large.\n\nWhat caught my eye, or better my ears, was a related radio news report which mentioned something along the lines that the submissions to the amendment reached a) a record number and b) that a considerable number of submissions were similar in their wording.\n\nI guess the former is indicative for the fundamental issues which the amendment touches and the overall somewhat 'edgy', not to say polarized atmosphere when it comes to Corona. The latter point, though, the similarity of submissions' wording, puzzled me a bit. What the similarity of the wording effectively means is the use of some kind of template by those filing a submission. In this regard, the news report mentioned that there have been some pertaining calls on social media channels to oppose the bill, possibly also including the provision of a template text.\n\nIt was this context which made me curious about the extent of the matter. At least curious enough to have an 'empirical' look at it with R.\n\nSo I went to the a) parliament's website which provides a record of received submissions, b) extracted the weblinks to all those submissions of which the text is public, c) download the pertaining pdfs, d) extracted their content, and e) finally checked for their similarity. As for the last point, I can gladly report that I learned something here and my approach changed a bit as I moved along. Hence, the blog post somewhat mirrors this process.\n\n```{r include=FALSE}\nsubmission_pattern <- \"Tür und Tor\"\n```\n\nWhen it comes to similarity, my first attempt was to simply look at the the recurrence of a distinct phrase which I repeatedly noticed while randomly glancing through some of the submissions. The formulation is *`r submission_pattern`* which means something like '(to open) the (flood)gates'. I guess it's safe to assume that those submissions articulated severe misgivings about the amendment.\n\nAs I'll show below, while this approach is already somewhat informative since it reveals a pervasive use of a rather distinct phrase and hence points towards the use of text template, it is rather inductive and likely to suffer from omissions of other, possibly even more re-current phrases. Furthermore, simply looking at one distinct phrase is likely to be a rather 'fragile' indicator. Only a minor, effectively non-substantive modification of the wording would result in missing out on similar submissions.\n\nTo make a long story short, this issue introduced me to the world of quantitative text analysis as provided by the powerful `quanteda` package. Admittedly, I am only starting to scratch the surface here, but hopefully deep enough to legitimately include it in this post. So, enough of context and waffling. In media R(es).\n\n# Get data\n\n## Links to submission subpages\n\nThe list of submissions filed for the amendment is provided [here](https://www.parlament.gv.at/PAKT/VHG/XXVII/ME/ME_00055/index.shtml#tab-Stellungnahmen){target=\"_blank\"} at the Parliament's website. To extract the links to the sub-pages which include the links to individual texts, I make us of the provided RSS feed. From there I extract the relevant elements and combine them to one dataframe containing the name of the person/institution filing a submission, the submission's date, and the link to the sub-page.\n\n```{r eval=FALSE, message=FALSE, warning=FALSE, include=TRUE, cache=T}\n#| code-summary: \"Code: get links to submission sub-pages\"\n\nlink_rss_all_submissions <- \"https://www.parlament.gv.at/PAKT/VHG/XXVII/ME/ME_00055/filter.psp?view=RSS&jsMode=&xdocumentUri=&filterJq=&view=&GP=XXVII&ITYP=ME&INR=55&SUCH=&listeId=142&FBEZ=FP_142\"\n\ndata <- xml2::read_xml(link_rss_all_submissions)\n\n#get link to subpages with link to submissions\ndf_submission_pages_link <- data %>% \n  xml2::xml_find_all(\"//link\") %>% \n  html_text() %>% \n  enframe(., \n          name=\"id\",\n          value=\"link_single_submission_page\") %>% \n  mutate(link_single_submission_page=str_squish(link_single_submission_page)) %>% \n  filter(id>2) #removes first two rows which don't include data on submissions\n\n#get title\ndf_submission_pages_name <- data %>% \n  xml2::xml_find_all(\"//title\") %>% \n  html_text() %>% \n  enframe(., \n          name=\"id\",\n          value=\"title\") %>% \n  mutate(name=str_extract(title, regex(\"(?<=\\\\>).*(?=\\\\<)\"))) %>% \n  filter(id>2) %>% \n  select(-title)\n\n#get publication date\ndf_submission_pages_pub_date <- data %>% \n  xml2::xml_find_all(\"//pubDate\") %>% \n  html_text() %>% \n  enframe(., \n          name=\"id\",\n          value=\"date\") %>% \n  mutate(date=date %>% str_squish() %>% lubridate::dmy_hms(., tz=\"Europe/Vienna\"))\n\n#combine to one dataframe\ndf_submission <- bind_cols(\n  df_submission_pages_name,\n  df_submission_pages_pub_date,\n  df_submission_pages_link,\n  ) %>% \n  select(-contains(\"id\"))\n```\n\n\n```{r eval=FALSE, include=FALSE}\nreadr::write_excel_csv2(df_submission, file=here::here(\"_blog_data\", \"2020-12-22-submissionsepidemiclaw\", \"df_submission_rss.csv\"))\n```\n\n```{r include=FALSE}\n# df_submission <- readr::read_csv2(file=\"df_submission_rss.csv\")\ndf_submission <- readr::read_csv2(file=here::here(\"posts\",\"2020-12-22-submissionsepidemiclaw\", \"df_submission_rss.csv\"))\n\n```\n\nAs it turns out, there were `r nrow(df_submission) %>% scales::comma()` submissions to the amendment in total.\n\n```{r echo=FALSE, paged.print=TRUE}\n#table.start\nreactable(df_submission, \n          defaultPageSize = 4, \n          columns = list(\n            link_single_submission_page = colDef(\n              cell = function(value) {\n                url <- value\n                htmltools::tags$a(href = url, \n                                  target = \"_blank\", \n                                  value)\n                },\n              width = 150,\n              html=TRUE\n              )),\n          filterable = TRUE,\n          bordered=TRUE,\n          compact = TRUE,\n          style = list(fontSize = \"10px\"),\n          theme = reactableTheme(\n            backgroundColor = plot_bg_color,\n                filterInputStyle = list(\n                  color=\"green\",\n                  backgroundColor = plot_bg_color)))\n#table.end\n```\n\nNotice that there are several submissions which state 'Keine öffentliche Stellungnahme' (no public statement) as a name. Hence, we will not be able to analyse the text of all submissions, but only of those which are made public. How many are there? Let's see:\n\n\n```{r message=FALSE, warning=FALSE}\n#| code-summary: \"Code: identify public and non-public submissions\"\n\ndf_submission <- df_submission %>% \n  mutate(public=case_when(str_detect(name, regex(\"Keine öffentliche Stellungnahme\",\n                                                 ignore_case = T)) ~ \"not public\",\n                          TRUE ~ as.character(\"public\")))\n\ntable(df_submission$public)\n```\n\n```{r echo=FALSE}\ntable(df_submission$public)\n```\n\nOut of `r nrow(df_submission) %>% scales::comma()` submissions, the text of `r df_submission %>% filter(public==\"public\") %>% nrow() %>% scales::comma()` is publicly available (no idea why some are published and others not, probably it's up to the submitter to decide). While the high number of non-public submissions is obviously quite a caveat for the further analysis, we still have the text of more than 50 % of all submissions to look into.\n\n## Links to pdfs\n\nNow let's use the links to the submissions' sub-pages and search in each sub-page for a link which leads to the actual pdf document of the submission. By using the browser's developer tools (F12 key), we can see the html code behind the page. Clicking on the link of one submission reveals that the link contains the word 'imfname'. I use this string to identify the links to the submissions' pdfs and neglect all other links and pdfs which are also on the site.\n\n![](identify_link.gif)\n\nTo do so, below the function which is applied to all (!) links leading to the sub-pages with the individual submissions. It reads the html code, extracts the links (href), converts the results into a dataframe, and subsequently filters out the links which include 'imfname'. I then complement the extracted link with the missing part of the web address (www.parlament....). Since there are several submission (sub-pages) which state that the text is not public, and don't include a link to a pdf, I insert in these cases a 'missing' instead of the link. To accelerate the process I apply the function in parallel by using \\`furrr::future\\_map\\`.\n\n```{r eval=FALSE, include=TRUE}\n#| code-summary: \"Code: read htmlof sub-pages, extract pdf-link\"\n\n# function to identify pdf link\nfn_get_doc_links <- function(x){ \n  \n  link_doc <- x %>% \n  read_html() %>% \n  html_nodes(\"a\") %>% \n  html_attr(\"href\") %>% \n  enframe(name=NULL,\n          value=\"link_part\") %>% \n  filter(str_detect(link_part, \"imfname\")) %>% \n  mutate(link_doc=paste0(\"https://www.parlament.gv.at\", link_part)) %>% \n        pull(link_doc)\n  ifelse(length(link_doc)>0, link_doc, \"missing\")\n  \n} \n\n#apply function to all links\ndf_submission <- df_submission %>% \n  mutate(link_doc=future_map_chr(link_single_submission_page, \n                          possibly(fn_get_doc_links,\n                                   otherwise=\"missing\"),\n                            .progress = T))\n```\n\n\n```{r eval=FALSE, include=FALSE}\n#check\ndf_submission %>% \n  filter(link_doc==\"missing\" & public==\"public\") %>% \n  nrow()\n#0 = ok; only links to non-public submissions are missing;\n\n#readr::write_excel_csv2(df_submission, path=here::here(\"data\", \"df_submission.csv\"))\n```\n\n## Download submissions\n\nNow with all pdf links available we can start to download them. To do so, I use the `walk2` function of the `purrr` package. Note that I also use `magrittr`'s `%$%`operator to expose/pipe into the the `walk2` function.\n\n```{r include=FALSE}\n# load saved links to pdf_files\n# df_submission <- readr::read_csv2(file=\"df_submission.csv\")\ndf_submission <- readr::read_csv2(file=here::here(\"posts\", \"2020-12-22-submissionsepidemiclaw\",\"df_submission.csv\"))\n\n```\n\n\n```{r eval=FALSE, include=TRUE}\n#| code-summary: \"Code: download files\"\n\n#define pdf's file name and download destination\ndf_submission<- df_submission %>% \n  mutate(file_name=str_extract(link_doc, regex(\"\\\\d+.pdf\"))) %>% \n  mutate(pdf_destination=glue::glue(\"{here::here('blog_data','2020-12-22-submissionsepidemiclaw','pdf_files')}/{file_name}\") %>% \n           as.character() %>% \n           str_trim(., side=c(\"both\")))\n\n#download\ndf_submission %>% \n  filter(link_doc!=\"missing\") %$% #exposes names\n  walk2(\n    link_doc,\n    pdf_destination,\n    download.file, \n    mode = \"wb\")\n```\n\n\nAfter the steps above we now have the all the available submission files at our disposal. Finally time to start working on the actual texts.\n\n## Extract text from pdfs\n\nWhen it comes to extracting text by means of OCR, `pdftools` by Jeroen Ooms is the package of choice. In preparation, we need to get the German language data. Extracting text from pdfs can be quite a time consuming matter. Going through the more than 3700 submissions initially took me more than 7 hours. Reducing the dpi value from 300 to 150 accelerated the procedure quite considerably, down to approx. 60 minutes, and I didn't notice any relevant decrease of the output's quality.\n\n\n```{r eval=FALSE, include=TRUE}\n#| code-summary: \"Code: extract text from pdfs\"\n\ntesseract::tesseract_download(lang = \"deu\")\ntesseract_info()\n\ndf_submission <- df_submission %>% \n   mutate(doc_text=map(pdf_destination, possibly(~pdftools::pdf_ocr_text(.,\n                                                                      language = \"deu\",\n                                                                      dpi=150),\n                                              otherwise=\"missing\")))\n```\n\n\n```{r include=FALSE}\ndf_submission <- readr::read_rds(file=here::here(\"posts\", \"2020-12-22-submissionsepidemiclaw\",\n                                                 \"df_submission_150.rds\"))\n\n```\n\nLet's clean up the imported text, i.e. collapse multi-page submissions into one single string text, remove leading and trailing white space etc.\n\n```{r}\n#| code-summary: \"Code: clean text\"\n\ndf_submission <- df_submission %>% \n  mutate(date=date %>% \n           str_squish() %>% \n           str_trim() %>% \n           lubridate::dmy_hms()) %>% \n  mutate(doc_text=map_chr(doc_text, paste, collapse=\" \")) %>% #collapse multipage docs\n  mutate(doc_text=str_squish(doc_text)) %>% \n  mutate(doc_text=stringr::str_trim(doc_text, side=c(\"both\"))) %>% \n  mutate(name_first=str_extract(name, regex(\"(?<=,\\\\s)[:alpha:]*(?=($|[^:alpha:]|\\\\s))\")), \n         .after=\"name\") \n```\n\n\nLet's pause for a moment and take stock: There are `r nrow(df_submission) %>% scales::comma()` submissions, out of which `r df_submission %>% filter(public==\"public\") %>% nrow() %>% scales::comma()` are public. From the latter we were able to extract `r df_submission %>% filter(doc_text!=\"missing\") %>% nrow() %>% scales::comma()`. Hence, `r df_submission %>% filter(public==\"public\") %>% nrow() - df_submission %>% filter(doc_text!=\"missing\") %>% nrow()` texts could not be retrieved.\n\n# Analysis\n\n## Identify submissions with key formulation\n\nNow with submissions' texts cleaned etc, let's identify those which contain the formulation which appeared quite frequently when randomly browsing through some of the texts: *`r submission_pattern`*.\n\n\n```{r include=T, eval=F}\n#| code-summary: \"Code: identify submission containing key phrase\"\nsubmission_pattern <- \"Tür und Tor\"\n```\n\n```{r}\ndf_submission <- df_submission %>% \n  mutate(pattern_indicator=stringr::str_detect(doc_text,\n                                                regex(pattern=submission_pattern,\n                                                      dotall=T,\n                                                      ignore_case=T,\n                                                      multiline=T))) %>%\n  mutate(overall_indicator=case_when(\n      public==\"not public\" ~ \"not public\",\n      pattern_indicator==TRUE ~ str_trunc(submission_pattern, 15),\n      TRUE ~ as.character(\"other\"))) %>% \n  mutate(doc_length=nchar(doc_text))\n```\n\n\n```{r}\n#| code-summary: \"Code: table with identified texts\"\n\ntb_text <- reactable(df_submission %>%\n            filter(pattern_indicator==TRUE) %>% \n            select(name, date, doc_text),\n          defaultPageSize = 4, \n          filterable = TRUE,\n          style = list(fontSize = \"10px\"),\n          columns=list(\n            name=colDef(width=100),\n            date=colDef(width=100),\n            doc_text=colDef(minWidth=200)),\n          bordered=TRUE,\n          compact = TRUE,\n          theme = reactableTheme(\n            backgroundColor = plot_bg_color,\n                filterInputStyle = list(\n                  color=\"green\",backgroundColor = plot_bg_color)\n          )\n          )\n\n```\n\n\n```{r echo=FALSE}\n#table.start\ntb_text\n#table.end\n```\n\n## Frequency of key formulation\n\n\n```{r message=FALSE, warning=FALSE}\n#| code-summary: \"Code: table\"\n# table -------------------------------------------------------------------\ndf_tb_submission_1 <- df_submission %>% \n  group_by(overall_indicator) %>% \n  summarise(n_abs=n()) %>% \n  mutate(n_rel=n_abs/sum(n_abs)) %>% \n  arrange(desc(n_abs)) %>% \n  mutate(indicator_tbl=case_when(overall_indicator==\"not public\" ~ \"Keine öffentliche Stellungnahmen\",\n                                 overall_indicator==str_trunc(submission_pattern, 15) ~ glue::glue(\"enthalten Formulierung '{submission_pattern}'\") %>% as.character(),\n                                 overall_indicator==\"other\" ~ \"andere öffentliche Stellungnahmen\"),\n         .before=1) %>% \n  select(-overall_indicator)\n```\n\n```{r message=FALSE, warning=FALSE}\ndf_tb_submission_2 <- df_submission %>% \n  group_by(overall_indicator) %>% \n  filter(overall_indicator!=\"not public\") %>% \n  summarise(n_abs=n()) %>% \n  mutate(n_rel=n_abs/sum(n_abs)) %>% \n  arrange(desc(n_abs)) %>% \n  mutate(indicator_tbl=case_when(overall_indicator==\"not public\" ~ \"Keine öffentliche Stellungnahmen\",\n                                 overall_indicator==str_trunc(submission_pattern, 15) ~ glue::glue(\"enthalten Formulierung '{submission_pattern}'\") %>% as.character(),\n                                 overall_indicator==\"other\" ~ \"andere öffentliche Stellungnahmen\"),\n         .before=1) %>% \n  select(-overall_indicator, -n_abs, n_rel_public=n_rel)\n```\n\n```{r message=FALSE, warning=FALSE}\ndf_tb_submission <- df_tb_submission_1 %>% \n  left_join(., df_tb_submission_2)\n```\n\n```{r message=FALSE, warning=FALSE}\ntb_submission <- df_tb_submission %>%   \n  gt() %>% \n  tab_header(\n    title = md(\"**Stellungnahmen zur Novelle des Epidemiegesetzes**\"),\n    subtitle = md(glue::glue(md(\" Von den {df_tb_submission %>% filter(!str_detect(indicator_tbl, 'Keine')) %>% summarise(n_abs=sum(n_abs)) %>% pull(n_abs) %>% scales::comma()} Stellungnahmen von denen der Text öffentlich ist enthalten rund {df_tb_submission %>% filter(str_detect(indicator_tbl, 'Formulierung')) %>% pull(n_rel_public) %>% scales::percent()} die Formulierung *'{submission_pattern}'*.\")))) %>% \n  tab_options(heading.align = \"left\",\n              row_group.font.weight = \"bold\",\n              table.background.color=plot_bg_color) %>% \n   tab_style(\n    style = list(\n      cell_fill(color = \"orange\"),\n      cell_text(weight = \"bold\")\n      ),\n    locations = cells_body(\n      columns = c(n_abs),\n      rows = n_abs == 2204) \n    )%>% \n       tab_style(\n        style = list(\n          cell_fill(color = \"orange\"),\n          cell_text(weight = \"bold\")\n        ),\n        locations = cells_body(\n        columns = c(n_abs, n_rel, n_rel_public),\n        rows = str_detect(indicator_tbl, \"Formulierung\")) \n    ) %>% \n  cols_label(indicator_tbl=\"Stellungnahme\",\n             n_abs=\"Anzahl\",\n             n_rel=\"Anteil alle St.\",\n             n_rel_public=\"Anteil öff. St.\") %>% \n  sub_missing(columns=c(n_rel_public), missing_text = \"--\") %>% \n  fmt_number(columns=c(n_abs), decimals=0) %>% \n  fmt_percent(columns = c(n_rel, n_rel_public), decimals = 1) %>% \n  tab_source_note(source_note = md(\"data: www.parlament.gv.at, 'Ministerialentwurf betreffend Bundesgesetz, mit dem das Epidemiegesetz 1950, das Tuberkulosegesetz und das COVID-19-Maßnahmengesetz geändert werden.'<br>analysis: Roland Schmidt | @zoowalk | https://werk.statt.codes\"))\n\n```\n\nAs the table below shows, in `r df_tb_submission %>% filter(str_detect(indicator_tbl, \"Formulierung\")) %>% pull(n_abs) %>% scales::comma()` submissions the formulation '*`r submission_pattern`*' appears. *This number amounts to `r df_tb_submission %>% filter(str_detect(indicator_tbl, \"Formulierung\")) %>% pull(n_rel) %>% scales::percent()` of all submissions, and to `r (df_tb_submission %>% filter(str_detect(indicator_tbl, \"Formulierung\")) %>% pull(n_abs)/(df_tb_submission %>% filter(str_detect(indicator_tbl, \"Formulierung\")) %>% pull(n_abs)+df_tb_submission %>% filter(str_detect(indicator_tbl, \"andere\")) %>% pull(n_abs))) %>% scales::percent()` of all submissions for which we can access the text.* I think that's quite remarkable in the sense that it is suggest for quite pervasive use of a submission template.\n\n```{r echo=FALSE, message=FALSE, warning=FALSE}\ntb_submission\n\n```\n\n## Distribution of length\n\nLet's approach the similarity of submissions from another angle now: The length of the text. The graph below shows the distribution of submissions' length in terms of number of characters. One density curve shows the distribution for submissions which include the key phrase '`r str_trunc(submission_pattern, 20)`', the other one shows the distribution of those which don't include it (and for which the text is available).\n\n```{r message=FALSE, warning=FALSE}\n#| code-summary: \"Code: distribution of length\"\n\n# distribution of length --------------------------------------------------\n\nmy_caption <- glue::glue(\"data: www.parlament.gv.at; Stellungnahmen zum Ministerialentwurf betreffend Bundesgesetz, mit dem das Epidemiegesetz 1950, das Tuberkulosegesetz \\nund das COVID-19-Maßnahmengesetz geändert werden.\\nanalysis: Roland Schmidt | @zoowalk | https://werk.statt.codes\")\n\npl_length <- df_submission %>% \n  filter(public!=\"not public\") %>%\n  filter(doc_length<1000) %>% #remove outliers\n  ggplot(., aes(y=pattern_indicator,\n             x=doc_length,\n             fill=pattern_indicator,\n             color=pattern_indicator)\n         )+\n  labs(title=paste(\"BEGUTACHTUNGSVERFAHREN EPIDEMIEGESETZ-NOVELLE:<br>\",\"Verteilung der Länge von Stellungnahmen mit u. ohne Formulierung \",\"'\",str_trunc(submission_pattern, 20),\"'\"),\n       subtitle=\"Zur besseren Sichtbarkeit wurden Stellungnahmen mit mehr als 1000 Zeichen ausgeschlossen.\",\n       x=\"Länge Stellungnahme in Zeichen\",\n       caption=my_caption)+\n  ggridges::geom_density_ridges(scale=5)+\n  scale_fill_manual(values=c(\"FALSE\"=\"darkolivegreen4\",\n                                   \"TRUE\"=\"orange\"),\n                    labels=c(\"FALSE\"=glue::glue(\"ohne '{str_trunc(submission_pattern, 20)}'\"),\n                             \"TRUE\"=glue::glue(\"mit '{str_trunc(submission_pattern, 20)}'\")))+\n  scale_color_manual(values=c(\"FALSE\"=\"darkolivegreen4\",\n                             \"TRUE\"=\"orange\"),\n                     labels=c(\"FALSE\"=glue::glue(\"ohne '{str_trunc(submission_pattern, 20)}'\"),\n                             \"TRUE\"=glue::glue(\"mit '{str_trunc(submission_pattern, 20)}'\")))+\n  scale_y_discrete(expand=expansion(mult=c(0, 0.1)))+\n  theme_post()+\n  theme(\n    plot.title.position = \"plot\",\n    panel.grid.minor.y = element_blank(),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    plot.caption = element_text(hjust=0),\n    legend.position = \"top\",\n    legend.justification = \"left\",\n   legend.direction = \"horizontal\",\n    legend.title=element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y=element_blank())+\n  guides(fill=guide_legend(reverse=T),\n         color=guide_legend(reverse=T))\n  \n```\n\n\n```{r echo=FALSE, dev='svg'}\npl_length\n```\n\nWhat the graph shows is that the length of submissions which contain the formulation '`r str_trunc(submission_pattern, 25)`' is concentrated at around 350 characters, while the length of other submissions is more dispersed. Hence, when it comes to the length of documents, those containing the distinct formulation are also pretty similar in term of their length. I take this as another indicator for the similarity of the submissions with our search phrase. As for the density curve of those not containing the search phrase, note that there is still a little 'bump' around 350. This could potentially reflect that we are missing out on some submissions which may originate from the same submission template, but do not include the exact wording of our search phrase.\n\n## Submissions over time\n\nI was also wondering whether there is any 'temporal' pattern as to the date of the submission. If the submissions were the result of a social media campaign, are the timings of their submissions clustered? This could be read as a reaction to e.g. a social media post providing a template and asking to submit it.\n\n```{r message=FALSE, warning=FALSE}\n#| code-summary: \"Code: submissions per day\"\n# bar graph ---------------------------------------------------------------\n\ndf_submission_pattern <- df_submission %>% \n  mutate(overall_indicator=fct_infreq(overall_indicator)) %>% \n  group_by(date, overall_indicator, .drop=F) %>% \n  summarise(n=n()) %>% \n  ungroup() %>% \n  mutate(date=as.Date(date))\n\nlevels(df_submission_pattern$overall_indicator)\n\nlabel_submission_pattern <- as.character(glue::glue(\"Enthält '{str_trunc(submission_pattern,15)}'\"))\n\ncolors_indicators=c(\"grey70\", \"orange\", \"darkolivegreen4\")\nnames(colors_indicators) <- c(\"not public\", str_trunc(submission_pattern, 15), \"other\")\n\npl_submission_pattern <- df_submission_pattern %>% \n  filter(date<=as.Date(\"2020-09-19\")) %>% \n  ggplot()+\n  labs(title=paste(\"BEGUTACHTUNGSVERFAHREN EPIDEMIEGESETZ-NOVELLE:<br>\",\"Zeitpunkt der Einrechung der Stellungnahmen, mit u. ohne Formulierung\"),\n       subtitle=\"\",\n       caption=my_caption,\n       x=\"Datum\",\n       y=\"Anzahl Einreichungen\")+\n  geom_bar(aes(x=date,\n               y=n,\n               fill=overall_indicator),\n           position=position_dodge2(),\n           color=NA,\n           stat=\"identity\",\n           key_glyph = \"dotplot\")+\n  scale_x_date(labels=scales::label_date_short(),\n               date_breaks=\"1 days\")+\n  scale_y_continuous(expand=expansion(mult=c(0,0.1)),\n                     labels = scales::label_comma())+\n  scale_fill_manual(values=colors_indicators,\n                    labels=c(\"not public\"=\"keine öffentliche Stellungnahme\",\n                             submission_pattern=label_submission_pattern,\n                                  \"other\"=\"andere öffentliche Stellungnahmen\"))+\n  theme_post()+\n  theme(\n    plot.title.position = \"panel\",\n    plot.subtitle = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    plot.caption = element_text(hjust=0),\n    legend.position = \"top\",\n    legend.box.margin = ggplot2::margin(l=0, unit=\"cm\"),\n    legend.margin = ggplot2::margin(l=0, unit=\"cm\"),\n    legend.key.size = unit(1, units=\"cm\"),\n    legend.justification = \"left\",\n    legend.title=element_blank(),\n    axis.title.x = element_blank()\n  )+\n  guides(fill=guide_legend(size=15))\n```\n\n\n```{r echo=FALSE, dev='svg'}\npl_submission_pattern\n```\n\nI think the bar chart above doesn't suggest any temporal clustering of submissions including '`r submission_pattern`'. But considering that the speed with which the amendment was introduced and the very short period to file submissions doesn't really allow for much clustering.\n\n# Analysis with 'quanteda'\n\nAs already indicated in the introduction, the above analysis - while informative to some extent - has its limitations. Limiting the search to the presence/absence of the key phrase '`r submission_pattern`' is prone to miss out on other wordings which might be even more prevalent in some of the submissions. Furthermore, only marginal changes to the wording could result in the omission of submissions which are otherwise very similar.\n\nTo overcome these limitations and approach the issue of text similarity in a methodologically more rigors manner, let's now take up some of the features of the `quanteda` package. I never worked with the package before but from what I can tell it provides a very comprehensive and powerful set of tools for quantitative text analysis (for an overview see [here](https://quanteda.io/articles/pkgdown/comparison.html){target=\"_blank\"}). \n\nTo be able to use the offered tools, we have to convert our collection of submission texts into a 'corpus'. Before doing so, I further clean the extracted texts so that we only have the actual text of the submission and not any header, footer, or meta-data. Then let's create a `corpus`.\n\n\n```{r}\n#| code-summary: \"Code: Clean text, create corpus\"\nlibrary(quanteda)\ndf_submission_quanteda <- df_submission %>% \n  filter(doc_text!=\"missing\") %>% \n  select(file_name, doc_text) %>% \n  mutate(doc_text=str_remove(doc_text,regex(\"^.*Eingebracht am: \\\\d{2}\\\\.\\\\d{2}\\\\.\\\\d{4}\\\\s*\"))) %>% \n  mutate(doc_text=str_remove(doc_text,regex(\"\\\\s?www.parlament.gv.at\\\\s*$\"))) %>% \n  mutate(doc_text=str_remove(doc_text,regex(\"^.?Betr\\\\.?\\\\:?\\\\s?Änderung(en)? des Ep(i|e)demie Gesetzes\\\\.?\"))) %>% \n  mutate(doc_text=doc_text %>% str_squish() %>% str_trim()) %>% \n  ungroup()\n\n#create vector\nvec_submission <- df_submission_quanteda %>% \n  deframe()\n\n#create corpus\ncorp_submission <- corpus(vec_submission, docvars=data.frame(submission=names(vec_submission)))\n```\n\n\nThe corpus is essentially a named vector with the names of the submission files the names of the vector's elements.\n\n```{r echo=FALSE}\nhead(corp_submission)\n```\n\n## Collocations\n\nWith this as the basis, we can now identify all collocations e.g. which comprise `r stringi::stri_count_words(submission_pattern)` words as e.g. in '`r submission_pattern`'. The `textstat_collocation` function provides us with the frequencies for all (!) collocations of the requested length. For performance reasons I limited the results to those collocations which result in more than 500 hits. For a detailed description of the function see [here](https://quanteda.io/reference/textstat_collocations.html){target=\"_blank\"}.\n\n\n```{r}\n#| code-summary: \"Code: Get collocations\"\n\n#collocations\n\nn_words <- str_count(submission_pattern, regex(\"\\\\S+\"))\nn_words\n\n\ncollocs <- textstat_collocations(\n  corp_submission,\n  method = \"lambda\",\n  size = n_words,\n  min_count = 500,\n  smoothing = 0.5,\n  tolower = T) %>% \n  arrange(desc(count))\n\ndf_collocs <- collocs %>% \n  as_tibble()\n\n```\n\n\nAnd as it turns out, our used inductively selected key phrase '`r submission_pattern`' is close to the most frequent one. The collocation '`r df_collocs %>% slice_head(n=1) %>% pull(collocation)`' appears `r df_collocs %>% slice_head(n=1) %>% pull(count) %>% scales::comma()`.[^1]\n\n[^1]: Note that the frequency of `r submission_pattern` quanteda result is different than the frequency I detected earlier. I am not entirely sure why this is, but I assume it's due to different approaches. While our result above is the actual\n\n```{r echo=FALSE}\n#table.start\nreactable(df_collocs,\n          bordered=TRUE,\n          compact = TRUE,\n          style = list(fontSize = \"10px\"),\n          filterable = TRUE,\n          theme = reactableTheme(\n            backgroundColor = plot_bg_color,\n                filterInputStyle = list(\n                  color=\"green\",\n                  backgroundColor = plot_bg_color)))\n#table.end\n```\n\n## Cosine similarity\n\nQuantitative text analysis offers also the concept of **cosine similarity** when it comes to evaluating the similarity of two or more documents. I won't dig into the details here, but want to refer you to the video below which provides in my view a clear and accessible overview.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/5lvS8078ykA\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n<br>\n\nOne of the convenient properties of the concept is that it allows us to compare documents of any lengths along a scale of 0 (not similar at all) to 1 (identical).\n\nIn order to control for different document length when it comes to check for similarity, we have to create a weighted document feature matrix which accounts for the relative frequency of words (see [here](https://quanteda.io/reference/dfm_weight.html?q=dfm%20_%20weight){target=\"_blank\"}).\n\n\n```{r}\n  #| code-summary: \"Code: create document feature matrix (dfm)\"\n#create document-feature matrix\n\nvec_submission_wo_pattern <- map_chr(vec_submission, ~str_remove_all(., submission_pattern))\n\ncorp_submission_wo_term <- corpus(vec_submission_wo_pattern, docvars=data.frame(submission=names(vec_submission_wo_pattern)))\n\n\ndfmat_submission <- dfm(corp_submission_wo_term, \n                        remove = stopwords(\"de\"),\n                        stem = TRUE, \n                        remove_punct = TRUE)\n\ndfmat_prop_submission <- dfm_weight(dfmat_submission,\n                                    scheme=\"prop\")\n```\n\nWith this we can calculate the cosine similarity of texts.\n```{r}\n#| code-summary: \"Code: create similarity matrix (dfm)\"\n#create similarity matrix\nmat_similarity <- textstat_simil(dfmat_prop_submission, dfmat_prop_submission, \n                            margin = \"documents\", \n                            method = \"cosine\")\n\n#convert to dataframe\ndf_similarity <- mat_similarity %>% \n  as.data.frame(., \n                upper=FALSE,\n                diag=FALSE,\n  ) %>% \n  mutate(sim_interval=cut(cosine, c(seq(0,1,.1)),\n                          right=F,\n                          include.lowest=T))\n```\n\n\n### Document pairwise comparison\n\nWhat we obtain now is a comparison of all submission pairs (!) as to their cosine similarity. Overall, these are `r df_similarity %>% nrow() %>% scales::comma()` pairs (duplicates removed, i.e. document 1 vs document 2 = document 2 vs document 1). Seems like text analysis can quickly get a bit large. To keep the table manageable I only show the first thousand.\n\n```{r echo=FALSE}\n#table.start\nreactable(head(df_similarity, n = 10^3),\n          columns=list(\n            cosine=colDef(format=colFormat(digits=4),\n                          align=\"left\")),\n          bordered=TRUE,\n          compact = TRUE,\n          style = list(fontSize = \"10px\"),\n          filterable = TRUE,\n          theme = reactableTheme(\n            backgroundColor = plot_bg_color,\n                filterInputStyle = list(\n                  color=\"green\",\n                  backgroundColor = plot_bg_color)))\n#table.end\n\n```\n\nTo be able to group these pairs according to their similarity, I cut their cosine similarity values into 10 intervals (`sim_interval`). We can now count the number of submission pairs per interval. The barplot below presents the relative result, i.e. the share of submission pairs  from the total number of pairs.\n\n\n```{r}\n#| code-summary: \"Code: Frequency of pairs per cosine interval \"\n\ndf_similarity <- df_similarity %>% \n  left_join(., df_submission %>% \n              select(file_name, pattern_indicator),\n            by=c(\"document1\"=\"file_name\")) %>% \n  rename(document1_indicator=pattern_indicator) %>% \n  left_join(., df_submission %>% \n              select(file_name, pattern_indicator),\n            by=c(\"document2\"=\"file_name\")) %>% \n  rename(document2_indicator=pattern_indicator) %>% \n  mutate(indicator=case_when(document1_indicator==T & document2_indicator==T ~ \"both submissions with key phrase\",\n                             document1_indicator==F & \n                               document2_indicator==F ~ \"both submissions without key phrase\",\n                             TRUE ~ as.character(\"one submission without key phrase\")))\n\nvec_color_submissions <- c(\"#FFC125\", \"#104E8B\", \"grey50\")\nnames(vec_color_submissions) <- c(\"both submissions with key phrase\",\n\"one submission without key phrase\",\n\"both submissions without key phrase\")\n\n#barplot\ndf_bar_sim <- df_similarity %>% \n  group_by(sim_interval, indicator) %>% \n  summarise(interval_n=n()) %>% \n  ungroup() %>% \n  mutate(interval_rel=interval_n/sum(interval_n)) %>% \n  ungroup() %>% \n  mutate(law=\"EpidemieG\") %>% \n  mutate(indicator=fct_relevel(indicator, names(vec_color_submissions)))\n\n\npl_bar_sim <- df_bar_sim %>% \n  ggplot() +\n  labs(\n    title=\"PUBLIC CONSULTATION PROCEDURE ON EPIDEMIC LAW:<br>\n    Distribution of all submission pairs' cosine similarity\",\n    subtitle=\"\",\n    y=\"% of all submission pairs\",\n    x=\"cosine similarity\",\n    caption=my_caption)+\n  geom_bar(aes(x=sim_interval,\n               y=interval_rel,\n               fill=indicator),\n           position=position_stack(),\n           stat=\"identity\")+\n  geom_text(\n    # data=. %>%\n    #           filter(sim_interval %in% c(\"[0.7,0.8)\", \"[0.8,0.9)\", \"[0.9,1]\")) %>%\n    #                    filter(str_detect(indicator, \"both submissions with key phrase\")),\ndata=. %>%  mutate(interval_rel_label=case_when(\n                                                indicator==\"both submissions without key phrase\" ~ NA_real_,\n  interval_rel>0.05 ~ interval_rel,\n                                                TRUE ~ NA_real_)),\n    aes(x=sim_interval,\n    y=interval_rel,\n    label=interval_rel_label %>% scales::percent(., accuracy = 0.01),\n    group=indicator),\nposition=position_stack(vjust=0.5, reverse = F),\ncolor=\"white\")+\n  # stat_summary(fun.y = sum, \n  #              aes(x=sim_interval,\n  #                  y=interval_rel+.1,\n  #                  label = ..y.. %>% scales::percent(., accuracy = 0.01), \n  #                  group = sim_interval), \n  #              # position=position_stack(),\n  #              # vjust=2,\n  #              geom = \"text\")+\n  geom_text(\n    aes(x=sim_interval,\n        y=interval_rel,\n        label = ..y.. %>% scales::percent(., accuracy = 0.01),\n        group = sim_interval), \n    stat = 'summary', \n    fun = sum, \n    vjust = -1\n  )+\n  scale_y_continuous(labels=scales::label_percent(accuracy = 1),\n                     expand=expansion(mult=c(0, 0.1)))+\n  scale_fill_manual(values=vec_color_submissions)+\n  theme_post()+\n  theme(axis.title.y = element_text(angle = 90,\n                                    size=9,\n                                    color=\"grey50\",\n                                    hjust=1),\n        legend.direction = \"horizontal\",\n        legend.position=\"top\",\n        plot.subtitle = element_blank(),\n        plot.title=element_markdown(margin=unit(0.5, \"cm\")),\n        legend.title=element_blank(),\n        legend.text = element_text(size=9))\n\n```\n\n```{r echo=FALSE, dev='svg'}\npl_bar_sim\n```\n<br>\nIf we take 0.7 as a cut-off point, we can conclude that `r scales::percent(df_similarity %>% filter(cosine>=.7) %>% nrow()/nrow(df_similarity), accuracy=0.01)` of all submission pairs (!) were very similar. Note that among these submission pairs, there is no pair in which not at least one document contains our key phrase *'`r submission_pattern`'*. On the other end of the scale, we see that the largest junk of non-similar pairs (`r df_bar_sim %>% filter(sim_interval==\"[0,0.1)\" & str_detect(indicator, \"one submission without\")) %>% pull(interval_rel) %>% scales::percent(accuracy=0.01)` in the interval [0, 0.1)) consists of pairs of one document with the key phrase and the other document without it. \n\nNote that I removed the key phrase before calculating the cosine similarity. Hence, the high degree of similarity of submission pairs containing the key phrase does not originate from the key phrase, but from the rest of the texts! \n\n```{r}\n#| code-summary: \"Code: Plot composition per cosine interval\"\n\ndf_bar_sim %>% \n  group_by(sim_interval, indicator) %>% \n              summarise(sum_interval=sum(interval_rel, na.rm=T)) %>% \n              mutate(sum_indidicator_rel=sum_interval/sum(sum_interval))\n```\n\nAnother way to approach the issue is to look into the composition of each interval. The plot below provides the pertaining overview. As it becomes quite clear, the the share of submission pairs with both documents containing our search phrase becomes higher the more similar the documents are. Again, let's note that the detected similarity does not originate from both documents containing '`r submission_pattern`' since I removed these words before calculating pairs' similarity. I take plot below as another indication of the use of a submission template. \n\n```{r message=FALSE, warning=FALSE}\npl_bar_sim_fill <- df_bar_sim %>% \n  ggplot() +\n  labs(\n    title=\"PUBLIC CONSULTATION PROCEDURE ON EPIDEMIC LAW:<br>\n    Composition of submission pairs' cosine similarity intervals\",\n    # subtitle=\"\",\n    y=\"% of all submission pairs\",\n    x=\"cosine similarity\",\n    caption=my_caption)+\n  geom_bar(aes(x=sim_interval,\n               y=interval_rel,\n               fill=indicator),\n           position=\"fill\",\n           stat=\"identity\")+\n  # geom_text(aes(x=sim_interval,\n  #               y=interval_rel,\n  #               group=indicator,\n  #               label=interval_rel %>% scales::percent(., accuracy = 0.01)),\n  #           position=position_fill(reverse=FALSE, vjust=0.5),\n  #           color=\"white\")+\n  geom_text(data=. %>% \n              group_by(sim_interval, indicator) %>% \n              summarise(sum_interval=sum(interval_rel, na.rm=T)) %>% \n              mutate(sum_indidicator_rel=sum_interval/sum(sum_interval)) %>% \n              mutate(sum_indidicator_rel=case_when(sum_indidicator_rel<0.05 ~ NA_real_,\n                                                   TRUE ~ as.numeric(sum_indidicator_rel))),\n            aes(x=sim_interval,\n                y=sum_indidicator_rel,\n                group=indicator,\n                label=sum_indidicator_rel %>% scales::percent(., accuracy=0.1)),\n            color=\"white\",\n            position=position_fill(vjust=0.5))+\n  geom_text(data=df_similarity %>% \n              group_by(sim_interval) %>% \n              summarise(n_obs=n()),\n            aes(x=sim_interval,\n                y=1.1,\n                label=n_obs %>% scales::comma()),\n            color=\"grey40\",\n            stat=\"identity\")+\n  geom_text(label=\"number of pairs:\",\n            x=.5,\n            y=1.17,\n            color=\"grey40\",\n            hjust=0,\n            check_overlap = T)+\n  scale_y_continuous(labels=scales::label_percent(accuracy=1),\n                     expand=expansion(mult=c(0, 0.1)),\n                     breaks=seq(0,1,.25))+\n  scale_fill_manual(values=vec_color_submissions)+\n  theme_post()+\n  theme(axis.title.y = element_text(angle = 90,\n                                    size=9,\n                                    color=\"grey50\",\n                                    hjust=1),\n        legend.position=\"top\",\n        legend.direction = \"horizontal\",\n        legend.text=element_text(size=9),\n        plot.subtitle = element_blank(),\n        plot.title=element_markdown(margin=unit(0.5, \"cm\")),\n        legend.title=element_blank())\n```\n\n\n```{r echo=FALSE, dev='svg'}\npl_bar_sim_fill\n```\n\n### Groupwise comparison\n\nFinally, to further highlight the similarity of documents with `r submission_pattern` in contrast to other submission pairs, let's contrast the distribution of of the cosine similarity values per group with a violin plot.\n\n\n```{r}\n#| code-summary: \"Code: groupwise comparison\"\n\n#create indicator for each document\ndf_submission <- df_submission %>%\n  mutate(search_term=str_detect(doc_text, regex(submission_pattern,\n                                                ignore_case = T,\n                                                multiline = T,\n                                                dotall = T))) \n\n#add indicator to pairwise similarity comparison for first document\ndf_similarity <- df_similarity %>% \n  left_join(.,\n            df_submission %>% \n              select(file_name, search_term),\n            by=c(\"document1\"=\"file_name\")) %>% \n  rename(document1_term=search_term)\n\n#add indicator to pairwise similarity comparison for first document\ndf_similarity <- df_similarity %>% \n  left_join(.,\n            df_submission %>% \n              select(file_name, search_term),\n            by=c(\"document2\"=\"file_name\")) %>% \n  rename(document2_term=search_term)\n\n#create one, both, none groups\ndf_similarity <- df_similarity %>% \n  mutate(group_id=case_when(\n    document1_term==TRUE & document2_term==TRUE ~ \"both\",\n    document1_term==FALSE & document2_term==FALSE ~ \"none\",\n    document1_term==TRUE & document2_term==FALSE ~ \"one\",\n    document1_term==FALSE & document2_term==TRUE ~ \"one\",\n    TRUE ~ as.character(\"missing\")))\n```\n\n```{r}\npl_sim_groups <- df_similarity %>% \n  mutate(indicator=fct_relevel(indicator, names(vec_color_submissions))) %>% \n  ggplot()+\n  labs(\n    title=\"PUBLIC CONSULTATION PROCEDURE ON EPIDEMIC LAW:<br>Cosine similarity of submission pairs per group\",\n    subtitle=str_wrap(glue::glue(\"Grouped by presence/absence of key phrase '{submission_pattern}'. Key phrase was removed before calculating cosine similarity.\"),110),\n    y=\"cosine similarity\",\n       caption=my_caption)+\n  geom_violin(aes(x=indicator,\n                   y=cosine,\n                  fill=indicator),\n              color=NA)+\n  theme_post()+\n  theme(legend.position = \"none\",\n        axis.title.x = element_blank())+\n  scale_fill_manual(values=vec_color_submissions)\n\n```\n\n\n\n```{r echo=FALSE, dev='svg'}\npl_sim_groups\n```\n\nI think the result shows quite clearly that those submissions which include the key phrase are much more similar to each other than all other groups of submission pairs even after having removed the key phrase. The graph also shows that those submission which do not contain the wording are hardly similar to each other.\n\n## Comparision with other bills\n\nBut how does this result contrast with e.g. submissions to other bills. To put the results for the epidemic bill into context, I repeated the analysis from above for another bill, the 2018/19 Fundamental Law on Social Welfare ('[Sozialhilfegrundgesetz](https://www.parlament.gv.at/PAKT/VHG/XXVI/ME/ME_00104/index.shtml#tab-Stellungnahmen){target=\"_blank\"}').[^2] The plot below contrasts the distribution of cosine similarities between submission pairs. \n\n[^2]: I don't reproduce the code for the analysis here, but it is available on my github.\n\n```{r echo=FALSE}\ndf_sozial_sim <- readr::read_csv2(file=here::here(\"posts\", \"2020-12-22-submissionsepidemiclaw\", \"df_bar_sim_sozial.csv\")) %>%  mutate(law=\"Sozialhilfegrundgesetz\")\n\ndf_sim_comb <- bind_rows(df_sozial_sim, df_bar_sim) %>% \n  group_by(law, sim_interval) %>% \n  summarise(interval_rel=sum(interval_rel, na.rm=T))\n\npl_sim <- df_sim_comb %>% \n  ungroup() %>% \n  ggplot() +\n  labs(title=\"PUBLIC CONSULTATION PROCEDURE:<br>Cosine similarity of submissions to \n        <span style=color:#FFC125>Epidemic Law</span> vs \n       <span style=color:firebrick '>Law on Social Welfare</span>\",\n       subtitle=\"Epidemic law = Epidemiegesetz; Law on Social Welfare = Sozialhilfegrundgesetz.\",\n    y=\"% of all submission pairs\",\n    x=\"cosine similarity\",\n    caption=glue::glue(str_wrap(\"data: www.parlament.gv.at; Stellungnahmen zum 'Ministerialentwurf betreffend Bundesgesetz, mit dem das Epidemiegesetz 1950, das Tuberkulosegesetz \nund das COVID-19-Maßnahmengesetz geändert werden' und 'Ministerialentwurf betreffend Bundesgesetze, mit dem ein Bundesgesetz betreffend Grundsätze für die Sozialhilfe (Sozialhilfe-Grundsatzgesetz) und ein Bundesgesetz über die bundesweite Gesamtstatistik über Leistungen der Sozialhilfe (Sozialhilfe-Statistikgesetz) erlassen werden.'\", 140), \"\\n\\nanalysis: Roland Schmidt | @zoowalk | https://werk.statt.codes\")) +\n  geom_bar(aes(x=sim_interval,\n               fill=law,\n               y=interval_rel),\n           position=position_dodge2(),\n           stat=\"identity\")+\n  scale_y_continuous(labels=scales::label_percent(),\n                     expand=expansion(mult=c(0, 0.1)))+\n  # scale_fill_paletteer_d(\"ggsci::default_jama\")[2:3]  +\n  scale_fill_manual(values=c(\"#FFC125\", \"firebrick\"))+\n  theme_post()+\n  theme(axis.title.y = element_text(angle = 90,\n                                    size=9,\n                                    color=\"grey50\",\n                                    hjust=1),\n        plot.title.position = \"panel\",\n        legend.position = \"none\",\n        legend.direction = \"horizontal\",\n        legend.title = element_blank())\n```\n\n```{r echo=FALSE, dev='svg'}\npl_sim\n```\n<br>\nI think the result is quite telling. In contrast to  epidemic law, the values pertaining to the Law on Social Welfare are almost normally distributed and centered on the [0.3, 0.4) interval. In other words, there almost no submissions which are particularly similar or dissimilar. I would think that this is a distribution typical for submissions without the (prevalent) presence of a text template. \n\n\n# Wrap up\n\nSo that's it (for now). Again, this got more lengthy than initially intended. A next step could be to see whether the similarity to submissions grew since the ability to file submissions via email. After all, copying a template, modifying it a bit before submission is not too difficult. Maybe as a final word, this is not to say that copy-and-paste submissions are of some sort of diminished democratic value. As always, if you have any comment or question regarding the post, feel free to contact me via DM on [twitter.](https://twitter.com/zoowalk){target=\"_blank\"}.\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"left","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["lightbox"],"css":["../../styles.css"],"highlight-style":"nord","toc":true,"toc-depth":4,"number-sections":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.43","editor":"visual","date-format":"D MMM YYYY","lightbox":{"match":"auto","desc-position":"bottom"},"theme":{"light":"litera"},"code-copy":true,"fontsize":"16px","toc-title":"Contents","toc-location":"left","fig-cap-location":"top","smooth-scroll":true,"author":[{"name":"Roland Schmidt"}],"title-block-banner":false,"license":"CC BY-NC","citation":true,"number-depth":3,"title":"Similarity of public submissions to Austria's amendment of the epidemic law","description":"An analysis of public submissions to bill seeking to amend Austria's epidemic law.","date":"12-22-2020","categories":["Austria","COVID","OCR","stringr","web scraping"],"image":"preview.png"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}